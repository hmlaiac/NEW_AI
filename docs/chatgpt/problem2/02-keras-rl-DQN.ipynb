{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "south-metabolism",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a href='http://www.pieriandata.com'><img src='../Pierian_Data_Logo.png'/></a>\n",
    "___\n",
    "<center><em>Copyright by Pierian Data Inc.</em></center>\n",
    "<center><em>For more information, visit us at <a href='http://www.pieriandata.com'>www.pieriandata.com</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-satisfaction",
   "metadata": {},
   "source": [
    "# Keras-RL DQN Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-firewall",
   "metadata": {},
   "source": [
    "## Introduction to keras-rl(2)\n",
    "\n",
    "In this notebook we will create our first Reinforcement Learning agent via keras-rl together,\n",
    "based on a simple task from open-ai gym, namely the *Cartpole Example*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-accident",
   "metadata": {},
   "source": [
    "At first we will import all necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "chronic-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # to reduce the game speed when playing manually\n",
    "\n",
    "import gym  # Contains the game we want to play\n",
    "from pyglet.window import key  # for manual playing\n",
    "\n",
    "# import necessary blocks from keras to build the Deep Learning backbone of our agent\n",
    "from keras.models import Sequential  # To compose multiple Layers\n",
    "from keras.layers import Dense  # Fully-Connected layer\n",
    "from keras.layers import Activation  # Activation functions\n",
    "from keras.layers import Flatten  # Flatten function\n",
    "\n",
    "from keras.optimizers import Adam  # Adam optimizer\n",
    "\n",
    "# Now the keras-rl2 agent. Dont get confused as it is only called rl and not keras-rl\n",
    "\n",
    "from rl.agents.dqn import DQNAgent  # Use the basic Deep-Q-Network agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-warehouse",
   "metadata": {},
   "source": [
    "Now we will create the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "uniform-ottawa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001B[0m\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\Desktop\\books\\django\\environment\\lib\\site-packages\\gym\\envs\\registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/56904270/difference-between-openai-gym-environments-cartpole-v0-and-cartpole-v1\n",
    "env_name = ENV_NAME = 'CartPole-v0'  # https://gym.openai.com/envs/CartPole-v1/\n",
    "env = gym.make(env_name)  # create the environment\n",
    "nb_actions = env.action_space.n  # get the number of possible actions\n",
    "print(nb_actions)  # Cartpole has only two possible actions: Either move left or right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-admission",
   "metadata": {},
   "source": [
    "Lets watch how the game looks when chosing random actions, or to be precise randomly move left and right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "naughty-membership",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "env.reset()  # reset the environment to the initial state\n",
    "for _ in range(200):  # play for max 200 iterations\n",
    "    env.render(mode=\"human\")  # render the current game state on your screen\n",
    "    random_action = env.action_space.sample()  # chose a random action\n",
    "    env.step(random_action)  # execute that action\n",
    "env.close()  # close the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-necessity",
   "metadata": {},
   "source": [
    "Now it is time that you try your luck! Try it out by using the left and right arrow key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "major-special",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'set_current'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 15\u001B[0m\n\u001B[0;32m     13\u001B[0m rewards \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1000\u001B[39m):\n\u001B[1;32m---> 15\u001B[0m     \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mhuman\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m     env\u001B[38;5;241m.\u001B[39mviewer\u001B[38;5;241m.\u001B[39mwindow\u001B[38;5;241m.\u001B[39mon_key_press \u001B[38;5;241m=\u001B[39m key_press  \u001B[38;5;66;03m# update the key press\u001B[39;00m\n\u001B[0;32m     17\u001B[0m     observation, reward, done, info \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action)\n",
      "File \u001B[1;32m~\\Desktop\\books\\django\\environment\\lib\\site-packages\\gym\\core.py:284\u001B[0m, in \u001B[0;36mWrapper.render\u001B[1;34m(self, mode)\u001B[0m\n\u001B[0;32m    283\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrender\u001B[39m(\u001B[38;5;28mself\u001B[39m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m--> 284\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\books\\django\\environment\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:137\u001B[0m, in \u001B[0;36mCartPoleEnv.render\u001B[1;34m(self, mode)\u001B[0m\n\u001B[0;32m    134\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcarttrans\u001B[38;5;241m.\u001B[39mset_translation(cartx, carty)\n\u001B[0;32m    135\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpoletrans\u001B[38;5;241m.\u001B[39mset_rotation(\u001B[38;5;241m-\u001B[39mx[\u001B[38;5;241m2\u001B[39m])\n\u001B[1;32m--> 137\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mviewer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreturn_rgb_array\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrgb_array\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\books\\django\\environment\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py:84\u001B[0m, in \u001B[0;36mViewer.render\u001B[1;34m(self, return_rgb_array)\u001B[0m\n\u001B[0;32m     82\u001B[0m glClearColor(\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     83\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwindow\u001B[38;5;241m.\u001B[39mclear()\n\u001B[1;32m---> 84\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwindow\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mswitch_to\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     85\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwindow\u001B[38;5;241m.\u001B[39mdispatch_events()\n\u001B[0;32m     86\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform\u001B[38;5;241m.\u001B[39menable()\n",
      "File \u001B[1;32m~\\Desktop\\books\\django\\environment\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py:361\u001B[0m, in \u001B[0;36mWin32Window.switch_to\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    360\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mswitch_to\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m--> 361\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mset_current\u001B[49m()\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'set_current'"
     ]
    }
   ],
   "source": [
    "action = 0\n",
    "def key_press(k, mod):\n",
    "    '''\n",
    "    This function gets the key press for gym\n",
    "    '''\n",
    "    global action\n",
    "    if k == key.LEFT:\n",
    "        action = 0\n",
    "    if k == key.RIGHT:\n",
    "        action = 1\n",
    "\n",
    "env.reset()\n",
    "rewards = 0\n",
    "for _ in range(1000):\n",
    "    env.render(mode=\"human\")\n",
    "    env.viewer.window.on_key_press = key_press  # update the key press\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    rewards+=1\n",
    "    if done:\n",
    "        print(f\"You got {rewards} points!\")\n",
    "        break\n",
    "    time.sleep(0.1)  # reduce speed a little bit\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-thinking",
   "metadata": {},
   "source": [
    "Let us build a Deep Neural Network and try if it can beat our score\n",
    "\n",
    "We use the same simple model with 2 hidden layers with 16 and 32 neurons each followed by relu activation\n",
    "\n",
    "The output layer has 2 nodes, one for each action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "essential-monaco",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 4)                 0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                80        \n",
      "                                                                 \n",
      " activation (Activation)     (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                544       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 690\n",
      "Trainable params: 690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# https://keras.io/api/layers/reshaping_layers/flatten/\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-spirituality",
   "metadata": {},
   "source": [
    "Lets create the DQN agent from keras-rl\n",
    "For this setting, the agent takes the following parameters:\n",
    "\n",
    "1. model = The model\n",
    "2. nb_actions = The number of actions (2 in this case)\n",
    "3. memory = The action replay memory. You can choose between the *SequentialMemory()* and *EpisodeParameterMemory() which is only used for one RL agent called CEM*\n",
    "4. nb_steps_warmup = How many iterations without training - Used to fill the memory\n",
    "5. target_model_update = When do we update the target model?\n",
    "6. Action Selection policy. You can choose between a *LinearAnnealedPolicy()*, *SoftmaxPolicy()*, *EpsGreedyQPolicy()*, *GreedyQPolicy()*, *GreedyQPolicy()*, *MaxBoltzmannQPolicy()* and *BoltzmannGumbelQPolicy()*. We use all of them during the next notebooks but feel free to try them out and inspect which works best here\n",
    "\n",
    "There are some more parameters, you can pass to the DQN Agent. Feel free to explore them, but we will also take a look at them together in the remaining notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-pleasure",
   "metadata": {},
   "source": [
    "Here we initialize the circular buffer with a limit of 20000 and a window length of 1.\n",
    "The window length describes the number of subsequent actions stored for a state.\n",
    "This will be demonstrated in the next lecture, when we start dealing with images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "residential-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.memory import SequentialMemory  # Sequential Memory for storing observations ( optimized circular buffer)\n",
    "\n",
    "memory = SequentialMemory(limit=20000, window_length=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-village",
   "metadata": {},
   "source": [
    "Then we define the Action Selection Policy: <br />\n",
    "We use *LinearAnnealedPolicy* in order to perform the epsilon greedy strategy with decaying epsilon. <br />\n",
    "*LinearAnnealedPolicy* accepts an action selection policy, its maximal and minimal values and a step number in order to create a dynimal policy. <br/>\n",
    "The minimal value epsilon can reach during training is 0.1.<br />\n",
    "For evaluation (e.g running the agent) it is fixed to 0.05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "toxic-attack",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearAnnealedPolicy allows to decay the epsilon for the epsilon greedy strategy\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
    "                              attr='eps',\n",
    "                              value_max=1.,\n",
    "                              value_min=.1,\n",
    "                              value_test=.05,\n",
    "                              nb_steps=20000) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-audience",
   "metadata": {},
   "source": [
    "Now we create the DQN Agent based on the defined model (**model**), the possible actions (**nb_actions**) (left and right in this case), the circular buffer (**memory**), the burnin or warmup phase (**10**), how often the target model gets updated (**100**) and the policy (**policy**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "piano-exercise",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "               target_model_update=100, policy=policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-habitat",
   "metadata": {},
   "source": [
    "Finally we compile our model with the Adam optimizer and a learning rate of 0.001.<br />\n",
    "We log the Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "opened-brooklyn",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\Desktop\\books\\django\\environment\\lib\\site-packages\\keras\\optimizers\\legacy\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Use learning_rate instead of lr if you get warning\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-rabbit",
   "metadata": {},
   "source": [
    "Now we run the training for 20000 steps. You can change visualize=True if you want to watch your model learning.\n",
    "Keep in mind that this increases the running time\n",
    "The training time is around 5 min so grep your favorite beverage and stay tuned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "rational-championship",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\Desktop\\books\\django\\environment\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    35/20000: episode: 1, duration: 0.860s, episode steps:  35, steps per second:  41, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.549003, mae: 0.583047, mean_q: 0.232418, mean_eps: 0.998988\n",
      "    46/20000: episode: 2, duration: 0.052s, episode steps:  11, steps per second: 213, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.447209, mae: 0.646520, mean_q: 0.554865, mean_eps: 0.998200\n",
      "    82/20000: episode: 3, duration: 0.188s, episode steps:  36, steps per second: 191, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 0.207129, mae: 0.664731, mean_q: 0.824566, mean_eps: 0.997143\n",
      "   127/20000: episode: 4, duration: 0.240s, episode steps:  45, steps per second: 187, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.238419, mae: 0.861196, mean_q: 1.161930, mean_eps: 0.995320\n",
      "   146/20000: episode: 5, duration: 0.093s, episode steps:  19, steps per second: 204, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 0.066350, mae: 1.041040, mean_q: 1.912541, mean_eps: 0.993880\n",
      "   158/20000: episode: 6, duration: 0.056s, episode steps:  12, steps per second: 214, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.089371, mae: 1.053433, mean_q: 2.017404, mean_eps: 0.993182\n",
      "   180/20000: episode: 7, duration: 0.107s, episode steps:  22, steps per second: 206, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.076924, mae: 1.025211, mean_q: 1.932607, mean_eps: 0.992418\n",
      "   204/20000: episode: 8, duration: 0.122s, episode steps:  24, steps per second: 197, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.129871, mae: 1.096299, mean_q: 2.010245, mean_eps: 0.991382\n",
      "   247/20000: episode: 9, duration: 0.217s, episode steps:  43, steps per second: 198, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 0.232685, mae: 1.590896, mean_q: 2.906324, mean_eps: 0.989875\n",
      "   278/20000: episode: 10, duration: 0.153s, episode steps:  31, steps per second: 202, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 0.104738, mae: 1.550881, mean_q: 3.045657, mean_eps: 0.988210\n",
      "   291/20000: episode: 11, duration: 0.066s, episode steps:  13, steps per second: 196, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.077119, mae: 1.542269, mean_q: 3.102924, mean_eps: 0.987220\n",
      "   301/20000: episode: 12, duration: 0.059s, episode steps:  10, steps per second: 170, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.202879, mae: 1.606284, mean_q: 3.074784, mean_eps: 0.986703\n",
      "   313/20000: episode: 13, duration: 0.065s, episode steps:  12, steps per second: 184, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.441161, mae: 1.997595, mean_q: 3.254399, mean_eps: 0.986208\n",
      "   330/20000: episode: 14, duration: 0.089s, episode steps:  17, steps per second: 191, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.309863, mae: 2.144035, mean_q: 4.154214, mean_eps: 0.985555\n",
      "   361/20000: episode: 15, duration: 0.163s, episode steps:  31, steps per second: 190, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 0.229720, mae: 2.058815, mean_q: 3.958033, mean_eps: 0.984475\n",
      "   386/20000: episode: 16, duration: 0.131s, episode steps:  25, steps per second: 192, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.171872, mae: 2.029902, mean_q: 3.975380, mean_eps: 0.983215\n",
      "   399/20000: episode: 17, duration: 0.065s, episode steps:  13, steps per second: 201, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.246365, mae: 2.072737, mean_q: 4.029006, mean_eps: 0.982360\n",
      "   417/20000: episode: 18, duration: 0.084s, episode steps:  18, steps per second: 215, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 0.559946, mae: 2.461700, mean_q: 4.273233, mean_eps: 0.981663\n",
      "   483/20000: episode: 19, duration: 0.355s, episode steps:  66, steps per second: 186, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.576 [0.000, 1.000],  loss: 0.309592, mae: 2.534738, mean_q: 4.889723, mean_eps: 0.979773\n",
      "   508/20000: episode: 20, duration: 0.115s, episode steps:  25, steps per second: 216, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.371621, mae: 2.676745, mean_q: 4.996201, mean_eps: 0.977725\n",
      "   545/20000: episode: 21, duration: 0.238s, episode steps:  37, steps per second: 155, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 0.350327, mae: 3.041543, mean_q: 5.883346, mean_eps: 0.976330\n",
      "   565/20000: episode: 22, duration: 0.182s, episode steps:  20, steps per second: 110, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.350 [0.000, 1.000],  loss: 0.455494, mae: 3.039379, mean_q: 5.859992, mean_eps: 0.975048\n",
      "   585/20000: episode: 23, duration: 0.177s, episode steps:  20, steps per second: 113, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.650 [0.000, 1.000],  loss: 0.274343, mae: 3.003019, mean_q: 5.828029, mean_eps: 0.974147\n",
      "   598/20000: episode: 24, duration: 0.074s, episode steps:  13, steps per second: 175, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.509697, mae: 3.026255, mean_q: 5.741105, mean_eps: 0.973405\n",
      "   621/20000: episode: 25, duration: 0.121s, episode steps:  23, steps per second: 190, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  loss: 0.626359, mae: 3.370837, mean_q: 6.204362, mean_eps: 0.972595\n",
      "   641/20000: episode: 26, duration: 0.105s, episode steps:  20, steps per second: 190, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.338530, mae: 3.382800, mean_q: 6.513516, mean_eps: 0.971627\n",
      "   657/20000: episode: 27, duration: 0.073s, episode steps:  16, steps per second: 219, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.420769, mae: 3.377483, mean_q: 6.461281, mean_eps: 0.970817\n",
      "   666/20000: episode: 28, duration: 0.043s, episode steps:   9, steps per second: 210, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.482171, mae: 3.376684, mean_q: 6.446723, mean_eps: 0.970255\n",
      "   691/20000: episode: 29, duration: 0.132s, episode steps:  25, steps per second: 190, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.412418, mae: 3.381086, mean_q: 6.476952, mean_eps: 0.969490\n",
      "   709/20000: episode: 30, duration: 0.106s, episode steps:  18, steps per second: 170, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.541901, mae: 3.551188, mean_q: 6.574801, mean_eps: 0.968523\n",
      "   718/20000: episode: 31, duration: 0.054s, episode steps:   9, steps per second: 166, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.588113, mae: 3.868333, mean_q: 7.403047, mean_eps: 0.967915\n",
      "   738/20000: episode: 32, duration: 0.113s, episode steps:  20, steps per second: 177, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.510719, mae: 3.782096, mean_q: 7.206545, mean_eps: 0.967263\n",
      "   751/20000: episode: 33, duration: 0.069s, episode steps:  13, steps per second: 188, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.481904, mae: 3.819185, mean_q: 7.332098, mean_eps: 0.966520\n",
      "   769/20000: episode: 34, duration: 0.092s, episode steps:  18, steps per second: 196, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 0.518794, mae: 3.783893, mean_q: 7.193899, mean_eps: 0.965823\n",
      "   826/20000: episode: 35, duration: 0.309s, episode steps:  57, steps per second: 185, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.589750, mae: 3.989011, mean_q: 7.586703, mean_eps: 0.964135\n",
      "   852/20000: episode: 36, duration: 0.175s, episode steps:  26, steps per second: 148, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.603934, mae: 4.200197, mean_q: 8.033506, mean_eps: 0.962267\n",
      "   872/20000: episode: 37, duration: 0.102s, episode steps:  20, steps per second: 196, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.429211, mae: 4.210846, mean_q: 8.160154, mean_eps: 0.961233\n",
      "   885/20000: episode: 38, duration: 0.062s, episode steps:  13, steps per second: 211, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.534607, mae: 4.168652, mean_q: 8.071231, mean_eps: 0.960490\n",
      "   913/20000: episode: 39, duration: 0.157s, episode steps:  28, steps per second: 178, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.607 [0.000, 1.000],  loss: 0.725479, mae: 4.339250, mean_q: 8.235545, mean_eps: 0.959568\n",
      "   955/20000: episode: 40, duration: 0.288s, episode steps:  42, steps per second: 146, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 0.839375, mae: 4.547097, mean_q: 8.700739, mean_eps: 0.957993\n",
      "   969/20000: episode: 41, duration: 0.071s, episode steps:  14, steps per second: 197, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.596915, mae: 4.606982, mean_q: 8.951408, mean_eps: 0.956733\n",
      "   984/20000: episode: 42, duration: 0.072s, episode steps:  15, steps per second: 208, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.674996, mae: 4.558149, mean_q: 8.842219, mean_eps: 0.956080\n",
      "   999/20000: episode: 43, duration: 0.109s, episode steps:  15, steps per second: 137, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.727777, mae: 4.525618, mean_q: 8.785194, mean_eps: 0.955405\n",
      "  1044/20000: episode: 44, duration: 0.351s, episode steps:  45, steps per second: 128, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.853333, mae: 4.901245, mean_q: 9.372093, mean_eps: 0.954055\n",
      "  1071/20000: episode: 45, duration: 0.148s, episode steps:  27, steps per second: 182, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 0.466522, mae: 4.912807, mean_q: 9.622027, mean_eps: 0.952435\n",
      "  1083/20000: episode: 46, duration: 0.064s, episode steps:  12, steps per second: 187, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.675605, mae: 4.896576, mean_q: 9.587860, mean_eps: 0.951557\n",
      "  1097/20000: episode: 47, duration: 0.082s, episode steps:  14, steps per second: 171, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.808160, mae: 4.866126, mean_q: 9.448888, mean_eps: 0.950973\n",
      "  1133/20000: episode: 48, duration: 0.182s, episode steps:  36, steps per second: 198, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 1.006795, mae: 5.304819, mean_q: 10.241127, mean_eps: 0.949848\n",
      "  1149/20000: episode: 49, duration: 0.073s, episode steps:  16, steps per second: 220, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.923862, mae: 5.338807, mean_q: 10.340584, mean_eps: 0.948678\n",
      "  1164/20000: episode: 50, duration: 0.078s, episode steps:  15, steps per second: 194, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.267 [0.000, 1.000],  loss: 0.931520, mae: 5.347643, mean_q: 10.377154, mean_eps: 0.947980\n",
      "  1202/20000: episode: 51, duration: 0.192s, episode steps:  38, steps per second: 198, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 0.819596, mae: 5.339996, mean_q: 10.415871, mean_eps: 0.946787\n",
      "  1220/20000: episode: 52, duration: 0.096s, episode steps:  18, steps per second: 188, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.722 [0.000, 1.000],  loss: 1.151381, mae: 5.909255, mean_q: 11.414761, mean_eps: 0.945527\n",
      "  1231/20000: episode: 53, duration: 0.062s, episode steps:  11, steps per second: 176, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 1.113203, mae: 5.741530, mean_q: 11.149421, mean_eps: 0.944875\n",
      "  1251/20000: episode: 54, duration: 0.120s, episode steps:  20, steps per second: 166, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.893869, mae: 5.737691, mean_q: 11.145918, mean_eps: 0.944178\n",
      "  1271/20000: episode: 55, duration: 0.104s, episode steps:  20, steps per second: 193, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 1.030692, mae: 5.879599, mean_q: 11.419521, mean_eps: 0.943277\n",
      "  1282/20000: episode: 56, duration: 0.053s, episode steps:  11, steps per second: 208, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 1.001703, mae: 5.815472, mean_q: 11.304080, mean_eps: 0.942580\n",
      "  1308/20000: episode: 57, duration: 0.141s, episode steps:  26, steps per second: 184, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 1.309973, mae: 5.830170, mean_q: 11.234554, mean_eps: 0.941748\n",
      "  1327/20000: episode: 58, duration: 0.113s, episode steps:  19, steps per second: 168, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.316 [0.000, 1.000],  loss: 1.299471, mae: 6.279344, mean_q: 12.155032, mean_eps: 0.940735\n",
      "  1337/20000: episode: 59, duration: 0.056s, episode steps:  10, steps per second: 180, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.850300, mae: 6.141419, mean_q: 12.059663, mean_eps: 0.940083\n",
      "  1368/20000: episode: 60, duration: 0.189s, episode steps:  31, steps per second: 164, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 1.224808, mae: 6.159017, mean_q: 12.036648, mean_eps: 0.939160\n",
      "  1394/20000: episode: 61, duration: 0.122s, episode steps:  26, steps per second: 214, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1.009410, mae: 6.117373, mean_q: 11.945706, mean_eps: 0.937878\n",
      "  1417/20000: episode: 62, duration: 0.119s, episode steps:  23, steps per second: 193, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.391 [0.000, 1.000],  loss: 1.457812, mae: 6.483422, mean_q: 12.559775, mean_eps: 0.936775\n",
      "  1430/20000: episode: 63, duration: 0.068s, episode steps:  13, steps per second: 192, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.915074, mae: 6.536799, mean_q: 12.904462, mean_eps: 0.935965\n",
      "  1447/20000: episode: 64, duration: 0.091s, episode steps:  17, steps per second: 187, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 1.624994, mae: 6.544583, mean_q: 12.711249, mean_eps: 0.935290\n",
      "  1479/20000: episode: 65, duration: 0.164s, episode steps:  32, steps per second: 195, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 1.230910, mae: 6.589646, mean_q: 12.906867, mean_eps: 0.934187\n",
      "  1497/20000: episode: 66, duration: 0.090s, episode steps:  18, steps per second: 201, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 1.298078, mae: 6.609379, mean_q: 12.914070, mean_eps: 0.933062\n",
      "  1509/20000: episode: 67, duration: 0.066s, episode steps:  12, steps per second: 181, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.277482, mae: 6.781562, mean_q: 13.122032, mean_eps: 0.932388\n",
      "  1544/20000: episode: 68, duration: 0.190s, episode steps:  35, steps per second: 184, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 1.220765, mae: 7.013357, mean_q: 13.801041, mean_eps: 0.931330\n",
      "  1555/20000: episode: 69, duration: 0.059s, episode steps:  11, steps per second: 187, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 1.533670, mae: 6.920598, mean_q: 13.542086, mean_eps: 0.930295\n",
      "  1566/20000: episode: 70, duration: 0.056s, episode steps:  11, steps per second: 197, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.508099, mae: 6.954506, mean_q: 13.833144, mean_eps: 0.929800\n",
      "  1588/20000: episode: 71, duration: 0.138s, episode steps:  22, steps per second: 159, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 1.158271, mae: 6.998871, mean_q: 13.720202, mean_eps: 0.929057\n",
      "  1617/20000: episode: 72, duration: 0.130s, episode steps:  29, steps per second: 223, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 2.070956, mae: 7.104842, mean_q: 13.694614, mean_eps: 0.927910\n",
      "  1632/20000: episode: 73, duration: 0.075s, episode steps:  15, steps per second: 201, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 2.034695, mae: 7.314354, mean_q: 14.228124, mean_eps: 0.926920\n",
      "  1644/20000: episode: 74, duration: 0.061s, episode steps:  12, steps per second: 197, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.532155, mae: 7.307041, mean_q: 14.363739, mean_eps: 0.926312\n",
      "  1658/20000: episode: 75, duration: 0.065s, episode steps:  14, steps per second: 215, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 1.739726, mae: 7.280399, mean_q: 14.260696, mean_eps: 0.925728\n",
      "  1672/20000: episode: 76, duration: 0.064s, episode steps:  14, steps per second: 217, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.712193, mae: 7.282338, mean_q: 14.553570, mean_eps: 0.925097\n",
      "  1690/20000: episode: 77, duration: 0.086s, episode steps:  18, steps per second: 209, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 1.758598, mae: 7.378539, mean_q: 14.422595, mean_eps: 0.924377\n",
      "  1716/20000: episode: 78, duration: 0.128s, episode steps:  26, steps per second: 203, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1.796279, mae: 7.607659, mean_q: 14.967650, mean_eps: 0.923387\n",
      "  1730/20000: episode: 79, duration: 0.069s, episode steps:  14, steps per second: 204, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.013240, mae: 7.695834, mean_q: 15.051120, mean_eps: 0.922488\n",
      "  1756/20000: episode: 80, duration: 0.154s, episode steps:  26, steps per second: 169, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.423 [0.000, 1.000],  loss: 1.699761, mae: 7.744398, mean_q: 15.272455, mean_eps: 0.921588\n",
      "  1796/20000: episode: 81, duration: 0.251s, episode steps:  40, steps per second: 159, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 1.736756, mae: 7.816065, mean_q: 15.422071, mean_eps: 0.920102\n",
      "  1809/20000: episode: 82, duration: 0.070s, episode steps:  13, steps per second: 185, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 1.762340, mae: 8.038169, mean_q: 15.718631, mean_eps: 0.918910\n",
      "  1825/20000: episode: 83, duration: 0.132s, episode steps:  16, steps per second: 121, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.651864, mae: 8.123855, mean_q: 15.946813, mean_eps: 0.918257\n",
      "  1851/20000: episode: 84, duration: 0.126s, episode steps:  26, steps per second: 206, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.577 [0.000, 1.000],  loss: 1.679983, mae: 8.099731, mean_q: 15.943338, mean_eps: 0.917313\n",
      "  1872/20000: episode: 85, duration: 0.103s, episode steps:  21, steps per second: 203, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 1.508808, mae: 8.102140, mean_q: 16.029866, mean_eps: 0.916255\n",
      "  1896/20000: episode: 86, duration: 0.126s, episode steps:  24, steps per second: 190, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.974574, mae: 8.239899, mean_q: 16.243468, mean_eps: 0.915243\n",
      "  1935/20000: episode: 87, duration: 0.199s, episode steps:  39, steps per second: 196, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 2.306882, mae: 8.367010, mean_q: 16.467171, mean_eps: 0.913825\n",
      "  1968/20000: episode: 88, duration: 0.164s, episode steps:  33, steps per second: 201, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.394 [0.000, 1.000],  loss: 2.215559, mae: 8.433286, mean_q: 16.536176, mean_eps: 0.912205\n",
      "  2012/20000: episode: 89, duration: 0.236s, episode steps:  44, steps per second: 186, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 1.733194, mae: 8.487511, mean_q: 16.784350, mean_eps: 0.910473\n",
      "  2029/20000: episode: 90, duration: 0.079s, episode steps:  17, steps per second: 214, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 1.835026, mae: 8.638682, mean_q: 17.241974, mean_eps: 0.909100\n",
      "  2055/20000: episode: 91, duration: 0.186s, episode steps:  26, steps per second: 140, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1.839657, mae: 8.786756, mean_q: 17.475198, mean_eps: 0.908133\n",
      "  2070/20000: episode: 92, duration: 0.073s, episode steps:  15, steps per second: 205, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 1.436965, mae: 8.742398, mean_q: 17.452207, mean_eps: 0.907210\n",
      "  2085/20000: episode: 93, duration: 0.075s, episode steps:  15, steps per second: 200, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 2.607895, mae: 8.736556, mean_q: 17.340783, mean_eps: 0.906535\n",
      "  2108/20000: episode: 94, duration: 0.129s, episode steps:  23, steps per second: 179, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 2.133985, mae: 8.880705, mean_q: 17.581156, mean_eps: 0.905680\n",
      "  2127/20000: episode: 95, duration: 0.088s, episode steps:  19, steps per second: 215, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 2.293973, mae: 9.129950, mean_q: 18.112345, mean_eps: 0.904735\n",
      "  2158/20000: episode: 96, duration: 0.146s, episode steps:  31, steps per second: 212, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 2.026410, mae: 9.069542, mean_q: 18.093941, mean_eps: 0.903610\n",
      "  2211/20000: episode: 97, duration: 0.259s, episode steps:  53, steps per second: 205, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.434 [0.000, 1.000],  loss: 1.847949, mae: 9.143278, mean_q: 18.258856, mean_eps: 0.901720\n",
      "  2245/20000: episode: 98, duration: 0.156s, episode steps:  34, steps per second: 218, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.262902, mae: 9.430160, mean_q: 18.952090, mean_eps: 0.899763\n",
      "  2289/20000: episode: 99, duration: 0.221s, episode steps:  44, steps per second: 199, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 2.388520, mae: 9.504077, mean_q: 19.013023, mean_eps: 0.898007\n",
      "  2328/20000: episode: 100, duration: 0.205s, episode steps:  39, steps per second: 190, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 2.127272, mae: 9.838128, mean_q: 19.630653, mean_eps: 0.896140\n",
      "  2358/20000: episode: 101, duration: 0.159s, episode steps:  30, steps per second: 189, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 2.216255, mae: 9.997942, mean_q: 20.139430, mean_eps: 0.894587\n",
      "  2435/20000: episode: 102, duration: 0.397s, episode steps:  77, steps per second: 194, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 2.230624, mae: 10.187345, mean_q: 20.364376, mean_eps: 0.892180\n",
      "  2445/20000: episode: 103, duration: 0.053s, episode steps:  10, steps per second: 188, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 2.643512, mae: 10.549305, mean_q: 21.088294, mean_eps: 0.890222\n",
      "  2491/20000: episode: 104, duration: 0.246s, episode steps:  46, steps per second: 187, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 2.690079, mae: 10.421318, mean_q: 20.822687, mean_eps: 0.888962\n",
      "  2514/20000: episode: 105, duration: 0.128s, episode steps:  23, steps per second: 179, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 2.651509, mae: 10.609677, mean_q: 21.271366, mean_eps: 0.887410\n",
      "  2556/20000: episode: 106, duration: 0.208s, episode steps:  42, steps per second: 202, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 2.654578, mae: 10.937432, mean_q: 22.021399, mean_eps: 0.885947\n",
      "  2593/20000: episode: 107, duration: 0.183s, episode steps:  37, steps per second: 202, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 3.524252, mae: 10.904749, mean_q: 21.786734, mean_eps: 0.884170\n",
      "  2630/20000: episode: 108, duration: 0.267s, episode steps:  37, steps per second: 138, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.568 [0.000, 1.000],  loss: 1.899406, mae: 11.148450, mean_q: 22.321673, mean_eps: 0.882505\n",
      "  2654/20000: episode: 109, duration: 0.150s, episode steps:  24, steps per second: 160, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 2.119853, mae: 11.144590, mean_q: 22.453959, mean_eps: 0.881132\n",
      "  2680/20000: episode: 110, duration: 0.234s, episode steps:  26, steps per second: 111, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 3.379503, mae: 11.219400, mean_q: 22.454954, mean_eps: 0.880008\n",
      "  2742/20000: episode: 111, duration: 0.395s, episode steps:  62, steps per second: 157, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 2.785000, mae: 11.528723, mean_q: 23.055209, mean_eps: 0.878028\n",
      "  2772/20000: episode: 112, duration: 0.134s, episode steps:  30, steps per second: 224, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.433 [0.000, 1.000],  loss: 3.393398, mae: 11.818123, mean_q: 23.619747, mean_eps: 0.875958\n",
      "  2786/20000: episode: 113, duration: 0.071s, episode steps:  14, steps per second: 198, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 2.150167, mae: 11.874444, mean_q: 23.880782, mean_eps: 0.874968\n",
      "  2801/20000: episode: 114, duration: 0.083s, episode steps:  15, steps per second: 181, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 2.840249, mae: 11.661775, mean_q: 23.356577, mean_eps: 0.874315\n",
      "  2846/20000: episode: 115, duration: 0.224s, episode steps:  45, steps per second: 201, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.578 [0.000, 1.000],  loss: 2.172794, mae: 12.352122, mean_q: 24.928583, mean_eps: 0.872965\n",
      "  2855/20000: episode: 116, duration: 0.047s, episode steps:   9, steps per second: 192, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 2.979162, mae: 12.108829, mean_q: 24.191131, mean_eps: 0.871750\n",
      "  2899/20000: episode: 117, duration: 0.211s, episode steps:  44, steps per second: 209, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.568 [0.000, 1.000],  loss: 3.527712, mae: 12.322764, mean_q: 24.665780, mean_eps: 0.870558\n",
      "  2927/20000: episode: 118, duration: 0.137s, episode steps:  28, steps per second: 204, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.027365, mae: 12.538829, mean_q: 25.212269, mean_eps: 0.868938\n",
      "  2946/20000: episode: 119, duration: 0.103s, episode steps:  19, steps per second: 184, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 1.826108, mae: 12.764389, mean_q: 25.944803, mean_eps: 0.867880\n",
      "  2980/20000: episode: 120, duration: 0.163s, episode steps:  34, steps per second: 209, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 1.889985, mae: 12.485908, mean_q: 25.486976, mean_eps: 0.866687\n",
      "  3001/20000: episode: 121, duration: 0.114s, episode steps:  21, steps per second: 184, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  loss: 1.509800, mae: 12.587912, mean_q: 25.642541, mean_eps: 0.865450\n",
      "  3032/20000: episode: 122, duration: 0.155s, episode steps:  31, steps per second: 200, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.323 [0.000, 1.000],  loss: 2.904438, mae: 12.964770, mean_q: 26.162923, mean_eps: 0.864280\n",
      "  3054/20000: episode: 123, duration: 0.115s, episode steps:  22, steps per second: 191, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 1.950501, mae: 13.020797, mean_q: 26.483270, mean_eps: 0.863088\n",
      "  3067/20000: episode: 124, duration: 0.066s, episode steps:  13, steps per second: 197, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 2.141409, mae: 12.822936, mean_q: 26.157695, mean_eps: 0.862300\n",
      "  3079/20000: episode: 125, duration: 0.062s, episode steps:  12, steps per second: 195, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 2.699927, mae: 13.169715, mean_q: 26.689051, mean_eps: 0.861737\n",
      "  3145/20000: episode: 126, duration: 0.328s, episode steps:  66, steps per second: 201, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.439 [0.000, 1.000],  loss: 2.237952, mae: 13.320009, mean_q: 27.154115, mean_eps: 0.859982\n",
      "  3200/20000: episode: 127, duration: 0.365s, episode steps:  55, steps per second: 151, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 1.948604, mae: 13.509113, mean_q: 27.435514, mean_eps: 0.857260\n",
      "  3268/20000: episode: 128, duration: 0.446s, episode steps:  68, steps per second: 152, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.981081, mae: 13.761331, mean_q: 27.836421, mean_eps: 0.854492\n",
      "  3301/20000: episode: 129, duration: 0.298s, episode steps:  33, steps per second: 111, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 3.232254, mae: 13.926716, mean_q: 28.261567, mean_eps: 0.852220\n",
      "  3344/20000: episode: 130, duration: 0.343s, episode steps:  43, steps per second: 125, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.442 [0.000, 1.000],  loss: 4.018229, mae: 14.202449, mean_q: 28.743585, mean_eps: 0.850510\n",
      "  3358/20000: episode: 131, duration: 0.098s, episode steps:  14, steps per second: 143, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 2.017479, mae: 14.169272, mean_q: 28.985393, mean_eps: 0.849227\n",
      "  3387/20000: episode: 132, duration: 0.176s, episode steps:  29, steps per second: 165, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 2.655330, mae: 14.373986, mean_q: 29.375722, mean_eps: 0.848260\n",
      "  3412/20000: episode: 133, duration: 0.137s, episode steps:  25, steps per second: 182, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 3.915018, mae: 14.524879, mean_q: 29.465685, mean_eps: 0.847045\n",
      "  3448/20000: episode: 134, duration: 0.208s, episode steps:  36, steps per second: 173, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.361 [0.000, 1.000],  loss: 2.499318, mae: 14.771357, mean_q: 29.981135, mean_eps: 0.845673\n",
      "  3468/20000: episode: 135, duration: 0.096s, episode steps:  20, steps per second: 208, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.350 [0.000, 1.000],  loss: 4.467429, mae: 14.735119, mean_q: 29.783651, mean_eps: 0.844413\n",
      "  3538/20000: episode: 136, duration: 0.331s, episode steps:  70, steps per second: 211, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.443 [0.000, 1.000],  loss: 3.642873, mae: 14.935402, mean_q: 30.275167, mean_eps: 0.842387\n",
      "  3605/20000: episode: 137, duration: 0.324s, episode steps:  67, steps per second: 207, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 3.075719, mae: 15.318990, mean_q: 31.144332, mean_eps: 0.839305\n",
      "  3636/20000: episode: 138, duration: 0.163s, episode steps:  31, steps per second: 190, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 3.530785, mae: 15.438420, mean_q: 31.236881, mean_eps: 0.837100\n",
      "  3676/20000: episode: 139, duration: 0.227s, episode steps:  40, steps per second: 176, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.425 [0.000, 1.000],  loss: 3.771256, mae: 15.543097, mean_q: 31.437215, mean_eps: 0.835502\n",
      "  3726/20000: episode: 140, duration: 0.236s, episode steps:  50, steps per second: 212, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.190841, mae: 15.405935, mean_q: 31.407240, mean_eps: 0.833478\n",
      "  3744/20000: episode: 141, duration: 0.096s, episode steps:  18, steps per second: 188, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.142014, mae: 16.143181, mean_q: 32.592852, mean_eps: 0.831947\n",
      "  3765/20000: episode: 142, duration: 0.102s, episode steps:  21, steps per second: 206, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 3.271362, mae: 16.000024, mean_q: 32.326368, mean_eps: 0.831070\n",
      "  3777/20000: episode: 143, duration: 0.061s, episode steps:  12, steps per second: 196, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 2.638317, mae: 15.625995, mean_q: 32.045803, mean_eps: 0.830328\n",
      "  3849/20000: episode: 144, duration: 0.328s, episode steps:  72, steps per second: 219, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 3.556252, mae: 16.277326, mean_q: 33.171974, mean_eps: 0.828438\n",
      "  3916/20000: episode: 145, duration: 0.352s, episode steps:  67, steps per second: 191, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 3.422618, mae: 16.392237, mean_q: 33.371566, mean_eps: 0.825310\n",
      "  3977/20000: episode: 146, duration: 0.311s, episode steps:  61, steps per second: 196, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 3.923302, mae: 16.845613, mean_q: 34.297205, mean_eps: 0.822430\n",
      "  4027/20000: episode: 147, duration: 0.310s, episode steps:  50, steps per second: 161, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 3.630160, mae: 17.196364, mean_q: 34.982231, mean_eps: 0.819932\n",
      "  4048/20000: episode: 148, duration: 0.130s, episode steps:  21, steps per second: 161, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 3.410015, mae: 17.151866, mean_q: 35.227331, mean_eps: 0.818335\n",
      "  4063/20000: episode: 149, duration: 0.128s, episode steps:  15, steps per second: 117, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.857790, mae: 17.241195, mean_q: 35.497015, mean_eps: 0.817525\n",
      "  4073/20000: episode: 150, duration: 0.085s, episode steps:  10, steps per second: 118, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 4.401169, mae: 17.158959, mean_q: 35.148555, mean_eps: 0.816963\n",
      "  4103/20000: episode: 151, duration: 0.196s, episode steps:  30, steps per second: 153, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.774741, mae: 16.861598, mean_q: 34.499832, mean_eps: 0.816063\n",
      "  4118/20000: episode: 152, duration: 0.094s, episode steps:  15, steps per second: 160, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 4.563601, mae: 17.199019, mean_q: 35.284266, mean_eps: 0.815050\n",
      "  4148/20000: episode: 153, duration: 0.184s, episode steps:  30, steps per second: 163, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 3.886026, mae: 17.915665, mean_q: 36.471800, mean_eps: 0.814038\n",
      "  4162/20000: episode: 154, duration: 0.209s, episode steps:  14, steps per second:  67, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.410099, mae: 18.029369, mean_q: 36.948812, mean_eps: 0.813047\n",
      "  4173/20000: episode: 155, duration: 0.072s, episode steps:  11, steps per second: 152, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 5.316287, mae: 17.787707, mean_q: 36.287120, mean_eps: 0.812485\n",
      "  4230/20000: episode: 156, duration: 0.343s, episode steps:  57, steps per second: 166, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.561 [0.000, 1.000],  loss: 3.791308, mae: 18.104524, mean_q: 37.071974, mean_eps: 0.810955\n",
      "  4252/20000: episode: 157, duration: 0.245s, episode steps:  22, steps per second:  90, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  loss: 2.804836, mae: 18.261426, mean_q: 37.626378, mean_eps: 0.809177\n",
      "  4310/20000: episode: 158, duration: 0.346s, episode steps:  58, steps per second: 168, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 4.393105, mae: 18.495955, mean_q: 37.674458, mean_eps: 0.807377\n",
      "  4319/20000: episode: 159, duration: 0.056s, episode steps:   9, steps per second: 160, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 4.252682, mae: 18.999983, mean_q: 38.611564, mean_eps: 0.805870\n",
      "  4349/20000: episode: 160, duration: 0.178s, episode steps:  30, steps per second: 169, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.567 [0.000, 1.000],  loss: 3.682442, mae: 18.632584, mean_q: 37.834770, mean_eps: 0.804993\n",
      "  4406/20000: episode: 161, duration: 0.400s, episode steps:  57, steps per second: 142, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 3.541338, mae: 18.613124, mean_q: 38.002759, mean_eps: 0.803035\n",
      "  4456/20000: episode: 162, duration: 0.300s, episode steps:  50, steps per second: 167, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 3.685026, mae: 19.269532, mean_q: 39.467052, mean_eps: 0.800628\n",
      "  4475/20000: episode: 163, duration: 0.123s, episode steps:  19, steps per second: 155, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 3.962330, mae: 18.866729, mean_q: 38.970117, mean_eps: 0.799075\n",
      "  4510/20000: episode: 164, duration: 0.237s, episode steps:  35, steps per second: 148, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 2.894822, mae: 19.589704, mean_q: 40.314639, mean_eps: 0.797860\n",
      "  4521/20000: episode: 165, duration: 0.066s, episode steps:  11, steps per second: 166, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 4.605716, mae: 19.999569, mean_q: 41.075688, mean_eps: 0.796825\n",
      "  4575/20000: episode: 166, duration: 0.329s, episode steps:  54, steps per second: 164, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 4.558561, mae: 20.124498, mean_q: 41.123978, mean_eps: 0.795363\n",
      "  4641/20000: episode: 167, duration: 0.390s, episode steps:  66, steps per second: 169, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 4.751956, mae: 20.277106, mean_q: 41.503685, mean_eps: 0.792663\n",
      "  4667/20000: episode: 168, duration: 0.194s, episode steps:  26, steps per second: 134, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 3.000398, mae: 20.263778, mean_q: 41.836181, mean_eps: 0.790593\n",
      "  4710/20000: episode: 169, duration: 0.281s, episode steps:  43, steps per second: 153, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 3.710505, mae: 20.492083, mean_q: 42.112513, mean_eps: 0.789040\n",
      "  4756/20000: episode: 170, duration: 0.283s, episode steps:  46, steps per second: 163, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 5.569411, mae: 21.050813, mean_q: 43.028533, mean_eps: 0.787038\n",
      "  4769/20000: episode: 171, duration: 0.094s, episode steps:  13, steps per second: 138, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 4.985073, mae: 21.138341, mean_q: 43.379611, mean_eps: 0.785710\n",
      "  4789/20000: episode: 172, duration: 0.150s, episode steps:  20, steps per second: 133, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 3.910257, mae: 21.084313, mean_q: 43.228516, mean_eps: 0.784967\n",
      "  4829/20000: episode: 173, duration: 0.239s, episode steps:  40, steps per second: 168, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.575 [0.000, 1.000],  loss: 3.207374, mae: 21.562813, mean_q: 44.305537, mean_eps: 0.783617\n",
      "  4914/20000: episode: 174, duration: 0.505s, episode steps:  85, steps per second: 168, episode reward: 85.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 3.493664, mae: 21.475911, mean_q: 44.253623, mean_eps: 0.780805\n",
      "  4940/20000: episode: 175, duration: 0.156s, episode steps:  26, steps per second: 166, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.698126, mae: 21.292716, mean_q: 43.983481, mean_eps: 0.778307\n",
      "  4962/20000: episode: 176, duration: 0.159s, episode steps:  22, steps per second: 139, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 2.531546, mae: 22.095782, mean_q: 45.580210, mean_eps: 0.777228\n",
      "  4978/20000: episode: 177, duration: 0.103s, episode steps:  16, steps per second: 155, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 3.083368, mae: 22.029270, mean_q: 45.039237, mean_eps: 0.776372\n",
      "  5001/20000: episode: 178, duration: 0.149s, episode steps:  23, steps per second: 154, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 3.121366, mae: 21.768399, mean_q: 45.209784, mean_eps: 0.775495\n",
      "  5044/20000: episode: 179, duration: 0.272s, episode steps:  43, steps per second: 158, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.581 [0.000, 1.000],  loss: 6.786136, mae: 22.444420, mean_q: 45.854164, mean_eps: 0.774010\n",
      "  5068/20000: episode: 180, duration: 0.149s, episode steps:  24, steps per second: 161, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 2.988412, mae: 22.714185, mean_q: 46.789548, mean_eps: 0.772502\n",
      "  5082/20000: episode: 181, duration: 0.091s, episode steps:  14, steps per second: 153, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.321545, mae: 21.858400, mean_q: 44.413806, mean_eps: 0.771648\n",
      "  5111/20000: episode: 182, duration: 0.209s, episode steps:  29, steps per second: 139, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 5.206945, mae: 22.627103, mean_q: 46.286763, mean_eps: 0.770680\n",
      "  5139/20000: episode: 183, duration: 0.204s, episode steps:  28, steps per second: 137, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.679 [0.000, 1.000],  loss: 4.529300, mae: 23.266780, mean_q: 47.641226, mean_eps: 0.769397\n",
      "  5220/20000: episode: 184, duration: 0.494s, episode steps:  81, steps per second: 164, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 4.795388, mae: 23.392154, mean_q: 47.958541, mean_eps: 0.766945\n",
      "  5270/20000: episode: 185, duration: 0.400s, episode steps:  50, steps per second: 125, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 3.761407, mae: 23.904290, mean_q: 49.133025, mean_eps: 0.763997\n",
      "  5337/20000: episode: 186, duration: 0.502s, episode steps:  67, steps per second: 133, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.567 [0.000, 1.000],  loss: 5.352271, mae: 24.033459, mean_q: 49.499561, mean_eps: 0.761365\n",
      "  5379/20000: episode: 187, duration: 0.273s, episode steps:  42, steps per second: 154, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 5.914564, mae: 24.131744, mean_q: 49.692730, mean_eps: 0.758912\n",
      "  5445/20000: episode: 188, duration: 0.501s, episode steps:  66, steps per second: 132, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 6.422239, mae: 24.640655, mean_q: 50.629887, mean_eps: 0.756482\n",
      "  5458/20000: episode: 189, duration: 0.166s, episode steps:  13, steps per second:  78, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 7.055039, mae: 24.223801, mean_q: 49.943127, mean_eps: 0.754705\n",
      "  5508/20000: episode: 190, duration: 0.339s, episode steps:  50, steps per second: 147, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.420 [0.000, 1.000],  loss: 6.005042, mae: 24.862388, mean_q: 51.231123, mean_eps: 0.753287\n",
      "  5533/20000: episode: 191, duration: 0.242s, episode steps:  25, steps per second: 103, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 6.208360, mae: 25.427245, mean_q: 52.413276, mean_eps: 0.751600\n",
      "  5579/20000: episode: 192, duration: 0.529s, episode steps:  46, steps per second:  87, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 5.398507, mae: 25.164032, mean_q: 51.830096, mean_eps: 0.750002\n",
      "  5592/20000: episode: 193, duration: 0.107s, episode steps:  13, steps per second: 121, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 9.654244, mae: 25.513680, mean_q: 52.459916, mean_eps: 0.748675\n",
      "  5632/20000: episode: 194, duration: 0.253s, episode steps:  40, steps per second: 158, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 4.017110, mae: 26.322827, mean_q: 54.205914, mean_eps: 0.747482\n",
      "  5682/20000: episode: 195, duration: 0.237s, episode steps:  50, steps per second: 211, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.627241, mae: 26.587499, mean_q: 54.720166, mean_eps: 0.745457\n",
      "  5701/20000: episode: 196, duration: 0.094s, episode steps:  19, steps per second: 203, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 4.942474, mae: 26.131909, mean_q: 53.818004, mean_eps: 0.743905\n",
      "  5773/20000: episode: 197, duration: 0.355s, episode steps:  72, steps per second: 203, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.174012, mae: 27.018809, mean_q: 55.570878, mean_eps: 0.741857\n",
      "  5784/20000: episode: 198, duration: 0.065s, episode steps:  11, steps per second: 169, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 6.417578, mae: 26.972576, mean_q: 55.251927, mean_eps: 0.739990\n",
      "  5818/20000: episode: 199, duration: 0.211s, episode steps:  34, steps per second: 161, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 8.560985, mae: 27.270432, mean_q: 55.979524, mean_eps: 0.738977\n",
      "  5927/20000: episode: 200, duration: 0.525s, episode steps: 109, steps per second: 208, episode reward: 109.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 8.592733, mae: 27.818060, mean_q: 57.000835, mean_eps: 0.735760\n",
      "  5985/20000: episode: 201, duration: 0.306s, episode steps:  58, steps per second: 189, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.431 [0.000, 1.000],  loss: 7.072315, mae: 27.944596, mean_q: 57.478462, mean_eps: 0.732003\n",
      "  6003/20000: episode: 202, duration: 0.098s, episode steps:  18, steps per second: 183, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 4.666720, mae: 28.335288, mean_q: 58.380100, mean_eps: 0.730293\n",
      "  6027/20000: episode: 203, duration: 0.134s, episode steps:  24, steps per second: 179, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 5.458814, mae: 28.792156, mean_q: 59.163372, mean_eps: 0.729347\n",
      "  6040/20000: episode: 204, duration: 0.078s, episode steps:  13, steps per second: 167, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 10.718684, mae: 27.989578, mean_q: 57.306628, mean_eps: 0.728515\n",
      "  6067/20000: episode: 205, duration: 0.208s, episode steps:  27, steps per second: 130, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.593 [0.000, 1.000],  loss: 9.194215, mae: 28.622544, mean_q: 58.965487, mean_eps: 0.727615\n",
      "  6150/20000: episode: 206, duration: 0.395s, episode steps:  83, steps per second: 210, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.554 [0.000, 1.000],  loss: 7.230938, mae: 29.027952, mean_q: 59.584510, mean_eps: 0.725140\n",
      "  6187/20000: episode: 207, duration: 0.190s, episode steps:  37, steps per second: 194, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.432 [0.000, 1.000],  loss: 5.515382, mae: 29.809172, mean_q: 61.320202, mean_eps: 0.722440\n",
      "  6241/20000: episode: 208, duration: 0.305s, episode steps:  54, steps per second: 177, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.252844, mae: 29.578442, mean_q: 60.987614, mean_eps: 0.720393\n",
      "  6290/20000: episode: 209, duration: 0.272s, episode steps:  49, steps per second: 180, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 8.947810, mae: 29.911131, mean_q: 61.582780, mean_eps: 0.718075\n",
      "  6364/20000: episode: 210, duration: 0.396s, episode steps:  74, steps per second: 187, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 7.018833, mae: 30.181418, mean_q: 62.174978, mean_eps: 0.715307\n",
      "  6410/20000: episode: 211, duration: 0.351s, episode steps:  46, steps per second: 131, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 8.375140, mae: 30.461424, mean_q: 62.709749, mean_eps: 0.712607\n",
      "  6434/20000: episode: 212, duration: 0.274s, episode steps:  24, steps per second:  87, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 9.119192, mae: 31.153661, mean_q: 63.904599, mean_eps: 0.711032\n",
      "  6449/20000: episode: 213, duration: 0.150s, episode steps:  15, steps per second: 100, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 9.187893, mae: 30.763413, mean_q: 62.894592, mean_eps: 0.710155\n",
      "  6474/20000: episode: 214, duration: 0.127s, episode steps:  25, steps per second: 197, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 7.905923, mae: 30.685199, mean_q: 62.772691, mean_eps: 0.709255\n",
      "  6486/20000: episode: 215, duration: 0.066s, episode steps:  12, steps per second: 181, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.477691, mae: 30.588835, mean_q: 63.131037, mean_eps: 0.708423\n",
      "  6504/20000: episode: 216, duration: 0.093s, episode steps:  18, steps per second: 194, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 6.850008, mae: 30.853473, mean_q: 63.426959, mean_eps: 0.707747\n",
      "  6612/20000: episode: 217, duration: 0.698s, episode steps: 108, steps per second: 155, episode reward: 108.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.472561, mae: 31.322162, mean_q: 64.340397, mean_eps: 0.704912\n",
      "  6751/20000: episode: 218, duration: 0.776s, episode steps: 139, steps per second: 179, episode reward: 139.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 8.789977, mae: 32.167345, mean_q: 66.140782, mean_eps: 0.699355\n",
      "  6813/20000: episode: 219, duration: 0.489s, episode steps:  62, steps per second: 127, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 7.889148, mae: 32.580697, mean_q: 67.000447, mean_eps: 0.694833\n",
      "  6853/20000: episode: 220, duration: 0.257s, episode steps:  40, steps per second: 155, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.425 [0.000, 1.000],  loss: 8.071101, mae: 33.945871, mean_q: 69.635278, mean_eps: 0.692537\n",
      "  6920/20000: episode: 221, duration: 0.384s, episode steps:  67, steps per second: 174, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 11.728469, mae: 33.945093, mean_q: 69.750294, mean_eps: 0.690130\n",
      "  6939/20000: episode: 222, duration: 0.096s, episode steps:  19, steps per second: 197, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 11.612505, mae: 33.848608, mean_q: 69.137593, mean_eps: 0.688195\n",
      "  6970/20000: episode: 223, duration: 0.167s, episode steps:  31, steps per second: 185, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 9.087064, mae: 34.499210, mean_q: 70.681712, mean_eps: 0.687070\n",
      "  6986/20000: episode: 224, duration: 0.091s, episode steps:  16, steps per second: 175, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 12.850834, mae: 34.069892, mean_q: 70.159015, mean_eps: 0.686012\n",
      "  7029/20000: episode: 225, duration: 0.220s, episode steps:  43, steps per second: 195, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 7.716659, mae: 35.370694, mean_q: 72.411317, mean_eps: 0.684685\n",
      "  7095/20000: episode: 226, duration: 0.350s, episode steps:  66, steps per second: 188, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 9.199538, mae: 34.657536, mean_q: 71.245226, mean_eps: 0.682233\n",
      "  7160/20000: episode: 227, duration: 0.352s, episode steps:  65, steps per second: 185, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 7.802580, mae: 35.366951, mean_q: 72.763213, mean_eps: 0.679285\n",
      "  7249/20000: episode: 228, duration: 0.435s, episode steps:  89, steps per second: 205, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 12.889870, mae: 36.027707, mean_q: 73.794773, mean_eps: 0.675820\n",
      "  7359/20000: episode: 229, duration: 0.525s, episode steps: 110, steps per second: 210, episode reward: 110.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 9.631843, mae: 36.764332, mean_q: 75.826488, mean_eps: 0.671343\n",
      "  7493/20000: episode: 230, duration: 0.640s, episode steps: 134, steps per second: 209, episode reward: 134.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 13.932069, mae: 37.453095, mean_q: 76.797895, mean_eps: 0.665853\n",
      "  7544/20000: episode: 231, duration: 0.242s, episode steps:  51, steps per second: 211, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.549 [0.000, 1.000],  loss: 14.127357, mae: 38.041486, mean_q: 78.217241, mean_eps: 0.661690\n",
      "  7612/20000: episode: 232, duration: 0.329s, episode steps:  68, steps per second: 206, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 13.731309, mae: 38.856226, mean_q: 79.622086, mean_eps: 0.659013\n",
      "  7747/20000: episode: 233, duration: 0.654s, episode steps: 135, steps per second: 207, episode reward: 135.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 13.587132, mae: 38.981382, mean_q: 80.086594, mean_eps: 0.654445\n",
      "  7766/20000: episode: 234, duration: 0.097s, episode steps:  19, steps per second: 197, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 17.791710, mae: 39.750814, mean_q: 81.361904, mean_eps: 0.650980\n",
      "  7808/20000: episode: 235, duration: 0.216s, episode steps:  42, steps per second: 194, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.795407, mae: 39.758758, mean_q: 81.402239, mean_eps: 0.649608\n",
      "  7831/20000: episode: 236, duration: 0.143s, episode steps:  23, steps per second: 160, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 16.173620, mae: 40.123848, mean_q: 81.731823, mean_eps: 0.648145\n",
      "  7880/20000: episode: 237, duration: 0.253s, episode steps:  49, steps per second: 194, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.449 [0.000, 1.000],  loss: 14.497261, mae: 39.722394, mean_q: 81.537695, mean_eps: 0.646525\n",
      "  7975/20000: episode: 238, duration: 0.462s, episode steps:  95, steps per second: 206, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 14.022177, mae: 41.105757, mean_q: 84.012322, mean_eps: 0.643285\n",
      "  8035/20000: episode: 239, duration: 0.299s, episode steps:  60, steps per second: 200, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 15.990858, mae: 40.619196, mean_q: 82.962063, mean_eps: 0.639797\n",
      "  8069/20000: episode: 240, duration: 0.170s, episode steps:  34, steps per second: 200, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.382 [0.000, 1.000],  loss: 20.344465, mae: 41.968473, mean_q: 85.503590, mean_eps: 0.637682\n",
      "  8112/20000: episode: 241, duration: 0.211s, episode steps:  43, steps per second: 204, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 9.947009, mae: 41.709183, mean_q: 85.347615, mean_eps: 0.635950\n",
      "  8164/20000: episode: 242, duration: 0.250s, episode steps:  52, steps per second: 208, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.442 [0.000, 1.000],  loss: 13.239918, mae: 41.820377, mean_q: 85.526770, mean_eps: 0.633813\n",
      "  8357/20000: episode: 243, duration: 0.931s, episode steps: 193, steps per second: 207, episode reward: 193.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 14.801281, mae: 42.211474, mean_q: 86.576218, mean_eps: 0.628300\n",
      "  8541/20000: episode: 244, duration: 0.879s, episode steps: 184, steps per second: 209, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 11.557341, mae: 43.540091, mean_q: 89.469642, mean_eps: 0.619818\n",
      "  8563/20000: episode: 245, duration: 0.111s, episode steps:  22, steps per second: 199, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  loss: 13.090197, mae: 45.016659, mean_q: 92.235184, mean_eps: 0.615182\n",
      "  8606/20000: episode: 246, duration: 0.218s, episode steps:  43, steps per second: 197, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 17.684151, mae: 44.591966, mean_q: 91.443479, mean_eps: 0.613720\n",
      "  8686/20000: episode: 247, duration: 0.375s, episode steps:  80, steps per second: 214, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 12.399426, mae: 44.663630, mean_q: 91.949226, mean_eps: 0.610953\n",
      "  8886/20000: episode: 248, duration: 0.949s, episode steps: 200, steps per second: 211, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 20.235342, mae: 46.194769, mean_q: 94.805482, mean_eps: 0.604652\n",
      "  8951/20000: episode: 249, duration: 0.436s, episode steps:  65, steps per second: 149, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 18.051651, mae: 47.350032, mean_q: 96.840971, mean_eps: 0.598690\n",
      "  9007/20000: episode: 250, duration: 0.330s, episode steps:  56, steps per second: 170, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 20.684341, mae: 47.708223, mean_q: 97.543053, mean_eps: 0.595968\n",
      "  9134/20000: episode: 251, duration: 0.666s, episode steps: 127, steps per second: 191, episode reward: 127.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 17.124801, mae: 48.136338, mean_q: 98.567516, mean_eps: 0.591850\n",
      "  9147/20000: episode: 252, duration: 0.139s, episode steps:  13, steps per second:  94, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 26.503289, mae: 48.363813, mean_q: 98.638184, mean_eps: 0.588700\n",
      "  9179/20000: episode: 253, duration: 0.227s, episode steps:  32, steps per second: 141, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 25.106004, mae: 48.404268, mean_q: 98.682914, mean_eps: 0.587687\n",
      "  9225/20000: episode: 254, duration: 0.386s, episode steps:  46, steps per second: 119, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 19.623442, mae: 49.177257, mean_q: 100.046098, mean_eps: 0.585933\n",
      "  9363/20000: episode: 255, duration: 0.796s, episode steps: 138, steps per second: 173, episode reward: 138.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 18.338579, mae: 49.659338, mean_q: 101.693935, mean_eps: 0.581792\n",
      "  9461/20000: episode: 256, duration: 0.520s, episode steps:  98, steps per second: 189, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 31.094205, mae: 49.906317, mean_q: 101.991589, mean_eps: 0.576483\n",
      "  9579/20000: episode: 257, duration: 0.640s, episode steps: 118, steps per second: 184, episode reward: 118.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 24.086501, mae: 51.110483, mean_q: 104.614963, mean_eps: 0.571622\n",
      "  9680/20000: episode: 258, duration: 0.533s, episode steps: 101, steps per second: 190, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 21.372212, mae: 52.022988, mean_q: 106.502314, mean_eps: 0.566695\n",
      "  9707/20000: episode: 259, duration: 0.137s, episode steps:  27, steps per second: 198, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 13.396853, mae: 51.859530, mean_q: 106.934964, mean_eps: 0.563815\n",
      "  9797/20000: episode: 260, duration: 0.472s, episode steps:  90, steps per second: 191, episode reward: 90.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 17.901347, mae: 52.605609, mean_q: 108.026592, mean_eps: 0.561182\n",
      "  9825/20000: episode: 261, duration: 0.148s, episode steps:  28, steps per second: 189, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 19.309247, mae: 53.577022, mean_q: 109.924180, mean_eps: 0.558527\n",
      "  9865/20000: episode: 262, duration: 0.213s, episode steps:  40, steps per second: 188, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 22.406839, mae: 53.928873, mean_q: 110.200628, mean_eps: 0.556997\n",
      " 10026/20000: episode: 263, duration: 0.856s, episode steps: 161, steps per second: 188, episode reward: 161.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 29.601732, mae: 53.954718, mean_q: 110.674730, mean_eps: 0.552475\n",
      " 10226/20000: episode: 264, duration: 1.091s, episode steps: 200, steps per second: 183, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 22.864257, mae: 54.977846, mean_q: 112.413115, mean_eps: 0.544353\n",
      " 10426/20000: episode: 265, duration: 1.057s, episode steps: 200, steps per second: 189, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 22.516867, mae: 55.966963, mean_q: 114.482370, mean_eps: 0.535353\n",
      " 10484/20000: episode: 266, duration: 0.310s, episode steps:  58, steps per second: 187, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 21.966178, mae: 56.389810, mean_q: 115.957286, mean_eps: 0.529547\n",
      " 10684/20000: episode: 267, duration: 1.055s, episode steps: 200, steps per second: 190, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 27.256400, mae: 57.571030, mean_q: 117.764209, mean_eps: 0.523742\n",
      " 10727/20000: episode: 268, duration: 0.236s, episode steps:  43, steps per second: 182, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.442 [0.000, 1.000],  loss: 14.932879, mae: 57.933196, mean_q: 118.645881, mean_eps: 0.518275\n",
      " 10800/20000: episode: 269, duration: 0.451s, episode steps:  73, steps per second: 162, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 28.034206, mae: 58.290185, mean_q: 119.729313, mean_eps: 0.515665\n",
      " 11000/20000: episode: 270, duration: 1.006s, episode steps: 200, steps per second: 199, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 29.156288, mae: 59.013067, mean_q: 120.789849, mean_eps: 0.509522\n",
      " 11043/20000: episode: 271, duration: 0.243s, episode steps:  43, steps per second: 177, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 27.582092, mae: 60.275690, mean_q: 122.785809, mean_eps: 0.504055\n",
      " 11059/20000: episode: 272, duration: 0.088s, episode steps:  16, steps per second: 183, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 47.782396, mae: 59.792061, mean_q: 121.625664, mean_eps: 0.502727\n",
      " 11174/20000: episode: 273, duration: 0.625s, episode steps: 115, steps per second: 184, episode reward: 115.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 33.518317, mae: 60.255437, mean_q: 122.741491, mean_eps: 0.499780\n",
      " 11374/20000: episode: 274, duration: 1.027s, episode steps: 200, steps per second: 195, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 28.535685, mae: 60.477745, mean_q: 123.401409, mean_eps: 0.492692\n",
      " 11418/20000: episode: 275, duration: 0.276s, episode steps:  44, steps per second: 159, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 34.805895, mae: 60.923365, mean_q: 125.031922, mean_eps: 0.487203\n",
      " 11453/20000: episode: 276, duration: 0.181s, episode steps:  35, steps per second: 193, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 31.479745, mae: 62.038568, mean_q: 126.227928, mean_eps: 0.485425\n",
      " 11589/20000: episode: 277, duration: 0.684s, episode steps: 136, steps per second: 199, episode reward: 136.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 35.955414, mae: 62.752554, mean_q: 127.839404, mean_eps: 0.481577\n",
      " 11735/20000: episode: 278, duration: 0.757s, episode steps: 146, steps per second: 193, episode reward: 146.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 39.550786, mae: 63.735261, mean_q: 129.942770, mean_eps: 0.475233\n",
      " 11912/20000: episode: 279, duration: 0.925s, episode steps: 177, steps per second: 191, episode reward: 177.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 31.904076, mae: 64.668779, mean_q: 132.279439, mean_eps: 0.467965\n",
      " 12066/20000: episode: 280, duration: 0.883s, episode steps: 154, steps per second: 174, episode reward: 154.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 33.318472, mae: 65.639058, mean_q: 134.381779, mean_eps: 0.460517\n",
      " 12265/20000: episode: 281, duration: 1.308s, episode steps: 199, steps per second: 152, episode reward: 199.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 28.357831, mae: 66.443086, mean_q: 135.812854, mean_eps: 0.452575\n",
      " 12440/20000: episode: 282, duration: 0.889s, episode steps: 175, steps per second: 197, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 28.416459, mae: 67.649459, mean_q: 138.072046, mean_eps: 0.444160\n",
      " 12490/20000: episode: 283, duration: 0.254s, episode steps:  50, steps per second: 197, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 30.267652, mae: 67.187314, mean_q: 137.299464, mean_eps: 0.439098\n",
      " 12690/20000: episode: 284, duration: 1.081s, episode steps: 200, steps per second: 185, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 31.582996, mae: 67.806449, mean_q: 138.341496, mean_eps: 0.433472\n",
      " 12793/20000: episode: 285, duration: 0.507s, episode steps: 103, steps per second: 203, episode reward: 103.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 33.563837, mae: 68.377313, mean_q: 139.483493, mean_eps: 0.426655\n",
      " 12944/20000: episode: 286, duration: 0.801s, episode steps: 151, steps per second: 188, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 30.741108, mae: 69.254064, mean_q: 141.371492, mean_eps: 0.420940\n",
      " 12960/20000: episode: 287, duration: 0.081s, episode steps:  16, steps per second: 198, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 64.215688, mae: 71.025452, mean_q: 144.743058, mean_eps: 0.417183\n",
      " 13160/20000: episode: 288, duration: 1.000s, episode steps: 200, steps per second: 200, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 30.787271, mae: 70.033840, mean_q: 142.843343, mean_eps: 0.412322\n",
      " 13350/20000: episode: 289, duration: 1.014s, episode steps: 190, steps per second: 187, episode reward: 190.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 36.732605, mae: 70.193599, mean_q: 143.433526, mean_eps: 0.403548\n",
      " 13391/20000: episode: 290, duration: 0.206s, episode steps:  41, steps per second: 199, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 57.514877, mae: 69.751004, mean_q: 142.362017, mean_eps: 0.398350\n",
      " 13591/20000: episode: 291, duration: 1.001s, episode steps: 200, steps per second: 200, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 33.128161, mae: 71.158748, mean_q: 145.340027, mean_eps: 0.392927\n",
      " 13742/20000: episode: 292, duration: 0.725s, episode steps: 151, steps per second: 208, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 37.351500, mae: 71.307993, mean_q: 145.649039, mean_eps: 0.385030\n",
      " 13859/20000: episode: 293, duration: 0.586s, episode steps: 117, steps per second: 200, episode reward: 117.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 44.378695, mae: 71.772261, mean_q: 146.214646, mean_eps: 0.379000\n",
      " 14037/20000: episode: 294, duration: 0.870s, episode steps: 178, steps per second: 205, episode reward: 178.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 36.834347, mae: 71.850582, mean_q: 146.688923, mean_eps: 0.372362\n",
      " 14150/20000: episode: 295, duration: 0.569s, episode steps: 113, steps per second: 199, episode reward: 113.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 43.536298, mae: 72.212538, mean_q: 147.093649, mean_eps: 0.365815\n",
      " 14350/20000: episode: 296, duration: 0.972s, episode steps: 200, steps per second: 206, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 35.911722, mae: 72.739876, mean_q: 148.710964, mean_eps: 0.358772\n",
      " 14479/20000: episode: 297, duration: 0.649s, episode steps: 129, steps per second: 199, episode reward: 129.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 42.272826, mae: 73.070175, mean_q: 148.901263, mean_eps: 0.351370\n",
      " 14679/20000: episode: 298, duration: 1.410s, episode steps: 200, steps per second: 142, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 33.835992, mae: 73.578012, mean_q: 149.900666, mean_eps: 0.343967\n",
      " 14879/20000: episode: 299, duration: 1.110s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 36.363553, mae: 74.063533, mean_q: 150.946105, mean_eps: 0.334967\n",
      " 14971/20000: episode: 300, duration: 0.455s, episode steps:  92, steps per second: 202, episode reward: 92.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 36.463303, mae: 74.731699, mean_q: 152.388814, mean_eps: 0.328397\n",
      " 15045/20000: episode: 301, duration: 0.393s, episode steps:  74, steps per second: 188, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 45.168787, mae: 75.648815, mean_q: 153.731571, mean_eps: 0.324662\n",
      " 15208/20000: episode: 302, duration: 0.844s, episode steps: 163, steps per second: 193, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 41.032562, mae: 75.636221, mean_q: 154.095166, mean_eps: 0.319330\n",
      " 15408/20000: episode: 303, duration: 1.679s, episode steps: 200, steps per second: 119, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 37.772296, mae: 76.425768, mean_q: 155.895342, mean_eps: 0.311162\n",
      " 15608/20000: episode: 304, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 51.702740, mae: 77.558501, mean_q: 157.732293, mean_eps: 0.302162\n",
      " 15808/20000: episode: 305, duration: 1.403s, episode steps: 200, steps per second: 143, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 36.400489, mae: 78.690824, mean_q: 160.342324, mean_eps: 0.293162\n",
      " 16008/20000: episode: 306, duration: 1.768s, episode steps: 200, steps per second: 113, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 40.626134, mae: 79.155716, mean_q: 161.301579, mean_eps: 0.284162\n",
      " 16198/20000: episode: 307, duration: 1.253s, episode steps: 190, steps per second: 152, episode reward: 190.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 46.856193, mae: 79.491985, mean_q: 162.343216, mean_eps: 0.275387\n",
      " 16398/20000: episode: 308, duration: 1.145s, episode steps: 200, steps per second: 175, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 47.254888, mae: 80.629339, mean_q: 163.853145, mean_eps: 0.266612\n",
      " 16598/20000: episode: 309, duration: 1.507s, episode steps: 200, steps per second: 133, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 42.415556, mae: 80.726854, mean_q: 164.336315, mean_eps: 0.257612\n",
      " 16798/20000: episode: 310, duration: 1.192s, episode steps: 200, steps per second: 168, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 48.522962, mae: 81.194696, mean_q: 165.344612, mean_eps: 0.248612\n",
      " 16998/20000: episode: 311, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 56.435462, mae: 81.706399, mean_q: 166.241650, mean_eps: 0.239612\n",
      " 17198/20000: episode: 312, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 58.565636, mae: 82.397512, mean_q: 167.786587, mean_eps: 0.230612\n",
      " 17398/20000: episode: 313, duration: 1.419s, episode steps: 200, steps per second: 141, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 45.412515, mae: 81.891373, mean_q: 166.558988, mean_eps: 0.221612\n",
      " 17598/20000: episode: 314, duration: 1.590s, episode steps: 200, steps per second: 126, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 44.037202, mae: 81.412504, mean_q: 165.356634, mean_eps: 0.212612\n",
      " 17798/20000: episode: 315, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 54.522585, mae: 81.279841, mean_q: 165.097399, mean_eps: 0.203612\n",
      " 17998/20000: episode: 316, duration: 1.170s, episode steps: 200, steps per second: 171, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 40.638150, mae: 81.708379, mean_q: 166.076580, mean_eps: 0.194612\n",
      " 18198/20000: episode: 317, duration: 0.993s, episode steps: 200, steps per second: 201, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 56.369108, mae: 82.035156, mean_q: 166.734827, mean_eps: 0.185612\n",
      " 18398/20000: episode: 318, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 51.583159, mae: 82.873377, mean_q: 168.468040, mean_eps: 0.176612\n",
      " 18598/20000: episode: 319, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 46.435900, mae: 82.378046, mean_q: 167.598996, mean_eps: 0.167612\n",
      " 18798/20000: episode: 320, duration: 1.496s, episode steps: 200, steps per second: 134, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 46.108607, mae: 82.704616, mean_q: 168.129104, mean_eps: 0.158612\n",
      " 18998/20000: episode: 321, duration: 1.426s, episode steps: 200, steps per second: 140, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 50.555405, mae: 83.497645, mean_q: 169.832299, mean_eps: 0.149612\n",
      " 19186/20000: episode: 322, duration: 1.013s, episode steps: 188, steps per second: 186, episode reward: 188.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 46.482876, mae: 84.140267, mean_q: 170.939569, mean_eps: 0.140882\n",
      " 19386/20000: episode: 323, duration: 1.155s, episode steps: 200, steps per second: 173, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 48.428191, mae: 84.977846, mean_q: 172.688096, mean_eps: 0.132152\n",
      " 19586/20000: episode: 324, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 52.634860, mae: 85.046002, mean_q: 172.752688, mean_eps: 0.123152\n",
      " 19786/20000: episode: 325, duration: 1.337s, episode steps: 200, steps per second: 150, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 48.202466, mae: 85.463643, mean_q: 173.825331, mean_eps: 0.114152\n",
      " 19986/20000: episode: 326, duration: 1.117s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 56.649384, mae: 86.028977, mean_q: 174.605082, mean_eps: 0.105152\n",
      "done, took 117.032 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x218ff50cdc0>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=20000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-alcohol",
   "metadata": {},
   "source": [
    "Wow! After only some minutes of training, we achieve great results!\n",
    "The reason for this is, that keras-rl has implemented many optimization strategies (e.g the optimized replay buffer) which lead to a much faster convergence than our DQN implemented by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-access",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training is done, we save the final weights.\n",
    "dqn.save_weights(f'dqn_{env_name}_weights.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-symphony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "dqn.test(env, nb_episodes=5, visualize=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-nirvana",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}