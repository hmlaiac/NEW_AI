{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "401ef339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4038c92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97ad2a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\Desktop\\books\\django\\environment\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001B[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001B[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(env_name)\n",
    "env.reset()  # reset the environment to the initial state\n",
    "for _ in range(200):  # play for max 200 iterations\n",
    "    env.render(mode=\"human\")  # render the current game state on your screen\n",
    "    random_action = env.action_space.sample()  # chose a random action\n",
    "    env.step(random_action)  # execute that action\n",
    "env.close()  # close the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38e81961",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_actions =env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a3d7221",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_obs = env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aa67996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(4,)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "640a808b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 4)                 0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                80        \n",
      "                                                                 \n",
      " activation (Activation)     (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                544       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 690\n",
      "Trainable params: 690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9c08e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f04304c",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=20000, window_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44e242d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cafae356",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(),\n",
    "                             attr='eps',\n",
    "                             value_max=1.0,\n",
    "                             value_min=0.1,\n",
    "                             value_test=0.05,\n",
    "                             nb_steps=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af35005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(model=model,\n",
    "               nb_actions=nb_actions,\n",
    "               memory=memory,\n",
    "               nb_steps_warmup=10,\n",
    "               target_model_update=100,\n",
    "               policy=policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3025354",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=0.001)\n",
    "import tensorflow as tf\n",
    "dqn.compile(optimizer=Adam(learning_rate=0.001),\n",
    "    metrics=['mae']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af9f8bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\Desktop\\books\\django\\environment\\lib\\site-packages\\keras\\engine\\training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "C:\\Users\\Thomas\\Desktop\\books\\django\\environment\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20/10000: episode: 1, duration: 0.865s, episode steps:  20, steps per second:  23, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.574686, mae: 0.600634, mean_q: 0.195825, mean_eps: 0.999325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\Desktop\\books\\django\\environment\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   36/10000: episode: 2, duration: 0.294s, episode steps:  16, steps per second:  54, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.467207, mae: 0.601295, mean_q: 0.374379, mean_eps: 0.998762\n",
      "   74/10000: episode: 3, duration: 0.636s, episode steps:  38, steps per second:  60, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  loss: 0.261169, mae: 0.609071, mean_q: 0.690888, mean_eps: 0.997548\n",
      "   91/10000: episode: 4, duration: 0.335s, episode steps:  17, steps per second:  51, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 0.098596, mae: 0.664253, mean_q: 1.070842, mean_eps: 0.996310\n",
      "  132/10000: episode: 5, duration: 0.724s, episode steps:  41, steps per second:  57, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.610 [0.000, 1.000],  loss: 0.288722, mae: 0.985243, mean_q: 1.415618, mean_eps: 0.995005\n",
      "  159/10000: episode: 6, duration: 0.458s, episode steps:  27, steps per second:  59, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.407 [0.000, 1.000],  loss: 0.108548, mae: 1.145789, mean_q: 2.126333, mean_eps: 0.993475\n",
      "  188/10000: episode: 7, duration: 0.484s, episode steps:  29, steps per second:  60, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 0.117738, mae: 1.123592, mean_q: 2.155805, mean_eps: 0.992215\n",
      "  205/10000: episode: 8, duration: 0.282s, episode steps:  17, steps per second:  60, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: 0.202018, mae: 1.213228, mean_q: 2.150508, mean_eps: 0.991180\n",
      "  230/10000: episode: 9, duration: 0.434s, episode steps:  25, steps per second:  58, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 0.275028, mae: 1.650811, mean_q: 2.930792, mean_eps: 0.990235\n",
      "  263/10000: episode: 10, duration: 0.549s, episode steps:  33, steps per second:  60, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.164255, mae: 1.649970, mean_q: 3.246535, mean_eps: 0.988930\n",
      "  276/10000: episode: 11, duration: 0.225s, episode steps:  13, steps per second:  58, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.152186, mae: 1.600617, mean_q: 3.205298, mean_eps: 0.987895\n",
      "  315/10000: episode: 12, duration: 0.659s, episode steps:  39, steps per second:  59, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 0.302056, mae: 1.836629, mean_q: 3.413146, mean_eps: 0.986725\n",
      "  359/10000: episode: 13, duration: 0.771s, episode steps:  44, steps per second:  57, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.568 [0.000, 1.000],  loss: 0.304693, mae: 2.224876, mean_q: 4.338438, mean_eps: 0.984858\n",
      "  382/10000: episode: 14, duration: 0.416s, episode steps:  23, steps per second:  55, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 0.311108, mae: 2.241917, mean_q: 4.299918, mean_eps: 0.983350\n",
      "  405/10000: episode: 15, duration: 0.397s, episode steps:  23, steps per second:  58, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.391 [0.000, 1.000],  loss: 0.363869, mae: 2.294438, mean_q: 4.235756, mean_eps: 0.982315\n",
      "  422/10000: episode: 16, duration: 0.282s, episode steps:  17, steps per second:  60, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 0.442221, mae: 2.715613, mean_q: 5.037139, mean_eps: 0.981415\n",
      "  464/10000: episode: 17, duration: 0.702s, episode steps:  42, steps per second:  60, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.384436, mae: 2.713063, mean_q: 5.201210, mean_eps: 0.980088\n",
      "  477/10000: episode: 18, duration: 0.216s, episode steps:  13, steps per second:  60, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.462420, mae: 2.763081, mean_q: 5.325377, mean_eps: 0.978850\n",
      "  498/10000: episode: 19, duration: 0.349s, episode steps:  21, steps per second:  60, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.383985, mae: 2.719938, mean_q: 5.166855, mean_eps: 0.978085\n",
      "  509/10000: episode: 20, duration: 0.182s, episode steps:  11, steps per second:  60, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.647629, mae: 3.015635, mean_q: 5.352275, mean_eps: 0.977365\n",
      "  529/10000: episode: 21, duration: 0.338s, episode steps:  20, steps per second:  59, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.466764, mae: 3.265698, mean_q: 6.323710, mean_eps: 0.976668\n",
      "  545/10000: episode: 22, duration: 0.347s, episode steps:  16, steps per second:  46, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.478300, mae: 3.178690, mean_q: 6.067620, mean_eps: 0.975858\n",
      "  564/10000: episode: 23, duration: 0.369s, episode steps:  19, steps per second:  52, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 0.419707, mae: 3.203655, mean_q: 6.216598, mean_eps: 0.975070\n",
      "  616/10000: episode: 24, duration: 0.915s, episode steps:  52, steps per second:  57, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 0.507004, mae: 3.307447, mean_q: 6.251026, mean_eps: 0.973472\n",
      "  630/10000: episode: 25, duration: 0.232s, episode steps:  14, steps per second:  60, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.699930, mae: 3.728328, mean_q: 7.170685, mean_eps: 0.971988\n",
      "  664/10000: episode: 26, duration: 0.585s, episode steps:  34, steps per second:  58, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.533533, mae: 3.585802, mean_q: 6.867526, mean_eps: 0.970908\n",
      "  722/10000: episode: 27, duration: 0.968s, episode steps:  58, steps per second:  60, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.569 [0.000, 1.000],  loss: 0.738691, mae: 3.780981, mean_q: 7.141609, mean_eps: 0.968837\n",
      "  740/10000: episode: 28, duration: 0.314s, episode steps:  18, steps per second:  57, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 0.799180, mae: 4.001171, mean_q: 7.471603, mean_eps: 0.967127\n",
      "  754/10000: episode: 29, duration: 0.232s, episode steps:  14, steps per second:  60, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.687519, mae: 4.019832, mean_q: 7.747169, mean_eps: 0.966408\n",
      "  769/10000: episode: 30, duration: 0.252s, episode steps:  15, steps per second:  59, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 0.711475, mae: 4.020737, mean_q: 7.710918, mean_eps: 0.965755\n",
      "  787/10000: episode: 31, duration: 0.320s, episode steps:  18, steps per second:  56, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.662613, mae: 4.044446, mean_q: 7.711413, mean_eps: 0.965013\n",
      "  821/10000: episode: 32, duration: 0.578s, episode steps:  34, steps per second:  59, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.441 [0.000, 1.000],  loss: 0.877571, mae: 4.216385, mean_q: 7.931214, mean_eps: 0.963843\n",
      "  846/10000: episode: 33, duration: 0.415s, episode steps:  25, steps per second:  60, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 1.101513, mae: 4.372399, mean_q: 8.227562, mean_eps: 0.962515\n",
      "  892/10000: episode: 34, duration: 0.788s, episode steps:  46, steps per second:  58, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 0.716574, mae: 4.374875, mean_q: 8.357386, mean_eps: 0.960917\n",
      "  923/10000: episode: 35, duration: 0.529s, episode steps:  31, steps per second:  59, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.613 [0.000, 1.000],  loss: 0.847633, mae: 4.595098, mean_q: 8.689486, mean_eps: 0.959185\n",
      "  954/10000: episode: 36, duration: 0.563s, episode steps:  31, steps per second:  55, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.581 [0.000, 1.000],  loss: 0.663337, mae: 4.681459, mean_q: 8.967929, mean_eps: 0.957790\n",
      "  992/10000: episode: 37, duration: 0.633s, episode steps:  38, steps per second:  60, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.506222, mae: 4.654128, mean_q: 9.028052, mean_eps: 0.956237\n",
      " 1010/10000: episode: 38, duration: 0.300s, episode steps:  18, steps per second:  60, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 1.045917, mae: 4.890804, mean_q: 9.214154, mean_eps: 0.954978\n",
      " 1054/10000: episode: 39, duration: 0.785s, episode steps:  44, steps per second:  56, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.998335, mae: 5.093024, mean_q: 9.818840, mean_eps: 0.953583\n",
      " 1096/10000: episode: 40, duration: 0.704s, episode steps:  42, steps per second:  60, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.002245, mae: 5.098972, mean_q: 9.812603, mean_eps: 0.951647\n",
      " 1128/10000: episode: 41, duration: 0.577s, episode steps:  32, steps per second:  55, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 1.077891, mae: 5.367800, mean_q: 10.254743, mean_eps: 0.949982\n",
      " 1176/10000: episode: 42, duration: 0.850s, episode steps:  48, steps per second:  56, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 0.849962, mae: 5.447613, mean_q: 10.609646, mean_eps: 0.948183\n",
      " 1196/10000: episode: 43, duration: 0.334s, episode steps:  20, steps per second:  60, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 0.734311, mae: 5.469008, mean_q: 10.676816, mean_eps: 0.946652\n",
      " 1247/10000: episode: 44, duration: 0.850s, episode steps:  51, steps per second:  60, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.431 [0.000, 1.000],  loss: 0.917850, mae: 5.759015, mean_q: 11.165467, mean_eps: 0.945055\n",
      " 1271/10000: episode: 45, duration: 0.400s, episode steps:  24, steps per second:  60, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 1.254465, mae: 5.797164, mean_q: 11.178201, mean_eps: 0.943367\n",
      " 1308/10000: episode: 46, duration: 0.617s, episode steps:  37, steps per second:  60, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.432 [0.000, 1.000],  loss: 1.012999, mae: 5.859437, mean_q: 11.331582, mean_eps: 0.941995\n",
      " 1323/10000: episode: 47, duration: 0.252s, episode steps:  15, steps per second:  59, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 1.157231, mae: 6.176355, mean_q: 12.037711, mean_eps: 0.940825\n",
      " 1345/10000: episode: 48, duration: 0.364s, episode steps:  22, steps per second:  61, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 1.295021, mae: 6.157900, mean_q: 11.932966, mean_eps: 0.939992\n",
      " 1369/10000: episode: 49, duration: 0.400s, episode steps:  24, steps per second:  60, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.496388, mae: 6.198940, mean_q: 12.020281, mean_eps: 0.938958\n",
      " 1402/10000: episode: 50, duration: 0.552s, episode steps:  33, steps per second:  60, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 1.180181, mae: 6.214384, mean_q: 12.060143, mean_eps: 0.937675\n",
      " 1421/10000: episode: 51, duration: 0.316s, episode steps:  19, steps per second:  60, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  loss: 1.451330, mae: 6.786546, mean_q: 13.150919, mean_eps: 0.936505\n",
      " 1452/10000: episode: 52, duration: 0.516s, episode steps:  31, steps per second:  60, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.419 [0.000, 1.000],  loss: 1.049779, mae: 6.598034, mean_q: 12.846485, mean_eps: 0.935380\n",
      " 1481/10000: episode: 53, duration: 0.483s, episode steps:  29, steps per second:  60, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 1.267327, mae: 6.645215, mean_q: 13.048613, mean_eps: 0.934030\n",
      " 1495/10000: episode: 54, duration: 0.235s, episode steps:  14, steps per second:  60, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 1.488340, mae: 6.612143, mean_q: 12.885905, mean_eps: 0.933062\n",
      " 1527/10000: episode: 55, duration: 0.536s, episode steps:  32, steps per second:  60, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 1.263927, mae: 6.878847, mean_q: 13.480666, mean_eps: 0.932028\n",
      " 1561/10000: episode: 56, duration: 0.580s, episode steps:  34, steps per second:  59, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.077653, mae: 6.935530, mean_q: 13.643274, mean_eps: 0.930543\n",
      " 1576/10000: episode: 57, duration: 0.254s, episode steps:  15, steps per second:  59, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.575480, mae: 6.889859, mean_q: 13.468943, mean_eps: 0.929440\n",
      " 1621/10000: episode: 58, duration: 0.763s, episode steps:  45, steps per second:  59, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 1.237909, mae: 7.154781, mean_q: 14.114675, mean_eps: 0.928090\n",
      " 1643/10000: episode: 59, duration: 0.367s, episode steps:  22, steps per second:  60, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  loss: 1.041187, mae: 7.256649, mean_q: 14.472784, mean_eps: 0.926583\n",
      " 1724/10000: episode: 60, duration: 1.349s, episode steps:  81, steps per second:  60, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 1.201244, mae: 7.534109, mean_q: 14.995164, mean_eps: 0.924265\n",
      " 1736/10000: episode: 61, duration: 0.200s, episode steps:  12, steps per second:  60, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 1.772818, mae: 7.695160, mean_q: 15.264641, mean_eps: 0.922172\n",
      " 1749/10000: episode: 62, duration: 0.220s, episode steps:  13, steps per second:  59, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 1.912468, mae: 7.919691, mean_q: 15.579201, mean_eps: 0.921610\n",
      " 1793/10000: episode: 63, duration: 0.748s, episode steps:  44, steps per second:  59, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 1.522875, mae: 7.935408, mean_q: 15.630415, mean_eps: 0.920327\n",
      " 1806/10000: episode: 64, duration: 0.214s, episode steps:  13, steps per second:  61, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 1.493031, mae: 8.077443, mean_q: 15.837291, mean_eps: 0.919045\n",
      " 1838/10000: episode: 65, duration: 0.534s, episode steps:  32, steps per second:  60, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.896135, mae: 8.342050, mean_q: 16.385842, mean_eps: 0.918033\n",
      " 1858/10000: episode: 66, duration: 0.333s, episode steps:  20, steps per second:  60, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.350 [0.000, 1.000],  loss: 0.838497, mae: 8.483205, mean_q: 16.861718, mean_eps: 0.916863\n",
      " 1875/10000: episode: 67, duration: 0.283s, episode steps:  17, steps per second:  60, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 1.450070, mae: 8.297889, mean_q: 16.482711, mean_eps: 0.916030\n",
      " 1894/10000: episode: 68, duration: 0.317s, episode steps:  19, steps per second:  60, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 1.599628, mae: 8.290657, mean_q: 16.337738, mean_eps: 0.915220\n",
      " 1919/10000: episode: 69, duration: 0.416s, episode steps:  25, steps per second:  60, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 1.654586, mae: 8.677129, mean_q: 17.129579, mean_eps: 0.914230\n",
      " 1932/10000: episode: 70, duration: 0.218s, episode steps:  13, steps per second:  60, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 2.198039, mae: 8.831126, mean_q: 17.377267, mean_eps: 0.913375\n",
      " 1952/10000: episode: 71, duration: 0.332s, episode steps:  20, steps per second:  60, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 1.467954, mae: 8.728424, mean_q: 17.387976, mean_eps: 0.912632\n",
      " 1964/10000: episode: 72, duration: 0.200s, episode steps:  12, steps per second:  60, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 1.691012, mae: 8.720078, mean_q: 17.269346, mean_eps: 0.911912\n",
      " 2024/10000: episode: 73, duration: 1.000s, episode steps:  60, steps per second:  60, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.820823, mae: 8.999958, mean_q: 17.779614, mean_eps: 0.910293\n",
      " 2035/10000: episode: 74, duration: 0.184s, episode steps:  11, steps per second:  60, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 2.345367, mae: 9.113701, mean_q: 18.072568, mean_eps: 0.908695\n",
      " 2050/10000: episode: 75, duration: 0.249s, episode steps:  15, steps per second:  60, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 1.952408, mae: 9.236627, mean_q: 18.446908, mean_eps: 0.908110\n",
      " 2070/10000: episode: 76, duration: 0.332s, episode steps:  20, steps per second:  60, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 2.325385, mae: 9.207414, mean_q: 18.233831, mean_eps: 0.907322\n",
      " 2088/10000: episode: 77, duration: 0.301s, episode steps:  18, steps per second:  60, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 1.712278, mae: 9.239354, mean_q: 18.304962, mean_eps: 0.906467\n",
      " 2110/10000: episode: 78, duration: 0.366s, episode steps:  22, steps per second:  60, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 2.565020, mae: 9.131048, mean_q: 17.971082, mean_eps: 0.905567\n",
      " 2157/10000: episode: 79, duration: 0.784s, episode steps:  47, steps per second:  60, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 1.833084, mae: 9.416509, mean_q: 18.797224, mean_eps: 0.904015\n",
      " 2192/10000: episode: 80, duration: 0.584s, episode steps:  35, steps per second:  60, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 1.789505, mae: 9.488432, mean_q: 18.994996, mean_eps: 0.902170\n",
      " 2235/10000: episode: 81, duration: 0.716s, episode steps:  43, steps per second:  60, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 1.747066, mae: 9.649730, mean_q: 19.253434, mean_eps: 0.900415\n",
      " 2263/10000: episode: 82, duration: 0.466s, episode steps:  28, steps per second:  60, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.028910, mae: 9.829853, mean_q: 19.690104, mean_eps: 0.898817\n",
      " 2274/10000: episode: 83, duration: 0.184s, episode steps:  11, steps per second:  60, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 2.891983, mae: 9.692689, mean_q: 19.384189, mean_eps: 0.897940\n",
      " 2286/10000: episode: 84, duration: 0.200s, episode steps:  12, steps per second:  60, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 1.300017, mae: 9.680474, mean_q: 19.491087, mean_eps: 0.897423\n",
      " 2300/10000: episode: 85, duration: 0.234s, episode steps:  14, steps per second:  60, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 1.195677, mae: 9.635718, mean_q: 19.536694, mean_eps: 0.896837\n",
      " 2369/10000: episode: 86, duration: 1.149s, episode steps:  69, steps per second:  60, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 2.187200, mae: 10.301697, mean_q: 20.513623, mean_eps: 0.894970\n",
      " 2405/10000: episode: 87, duration: 0.600s, episode steps:  36, steps per second:  60, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.934766, mae: 10.382263, mean_q: 20.697707, mean_eps: 0.892607\n",
      " 2414/10000: episode: 88, duration: 0.150s, episode steps:   9, steps per second:  60, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 2.809324, mae: 10.986798, mean_q: 21.957865, mean_eps: 0.891595\n",
      " 2428/10000: episode: 89, duration: 0.233s, episode steps:  14, steps per second:  60, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 1.882670, mae: 10.685090, mean_q: 21.456285, mean_eps: 0.891077\n",
      " 2445/10000: episode: 90, duration: 0.285s, episode steps:  17, steps per second:  60, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 1.915678, mae: 10.963318, mean_q: 21.989503, mean_eps: 0.890380\n",
      " 2461/10000: episode: 91, duration: 0.264s, episode steps:  16, steps per second:  61, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 2.384693, mae: 10.658023, mean_q: 21.227375, mean_eps: 0.889638\n",
      " 2509/10000: episode: 92, duration: 0.802s, episode steps:  48, steps per second:  60, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.879549, mae: 10.682336, mean_q: 21.481953, mean_eps: 0.888197\n",
      " 2558/10000: episode: 93, duration: 0.816s, episode steps:  49, steps per second:  60, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 1.772588, mae: 11.228717, mean_q: 22.666825, mean_eps: 0.886015\n",
      " 2596/10000: episode: 94, duration: 0.635s, episode steps:  38, steps per second:  60, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 2.038724, mae: 11.241006, mean_q: 22.692110, mean_eps: 0.884058\n",
      " 2639/10000: episode: 95, duration: 0.716s, episode steps:  43, steps per second:  60, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 2.209766, mae: 11.576745, mean_q: 23.186439, mean_eps: 0.882235\n",
      " 2670/10000: episode: 96, duration: 0.517s, episode steps:  31, steps per second:  60, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 2.719605, mae: 11.614406, mean_q: 23.347763, mean_eps: 0.880570\n",
      " 2695/10000: episode: 97, duration: 0.416s, episode steps:  25, steps per second:  60, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 2.358621, mae: 11.661469, mean_q: 23.399045, mean_eps: 0.879310\n",
      " 2747/10000: episode: 98, duration: 0.872s, episode steps:  52, steps per second:  60, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.596 [0.000, 1.000],  loss: 2.811096, mae: 11.925717, mean_q: 23.866631, mean_eps: 0.877577\n",
      " 2762/10000: episode: 99, duration: 0.262s, episode steps:  15, steps per second:  57, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 2.137518, mae: 11.982235, mean_q: 24.098226, mean_eps: 0.876070\n",
      " 2816/10000: episode: 100, duration: 0.900s, episode steps:  54, steps per second:  60, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 2.559800, mae: 12.074151, mean_q: 24.235556, mean_eps: 0.874518\n",
      " 2897/10000: episode: 101, duration: 1.352s, episode steps:  81, steps per second:  60, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 2.533203, mae: 12.302465, mean_q: 24.796560, mean_eps: 0.871480\n",
      " 2916/10000: episode: 102, duration: 0.316s, episode steps:  19, steps per second:  60, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 2.613057, mae: 12.568800, mean_q: 25.229585, mean_eps: 0.869230\n",
      " 2939/10000: episode: 103, duration: 0.383s, episode steps:  23, steps per second:  60, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  loss: 2.989410, mae: 12.449681, mean_q: 25.232874, mean_eps: 0.868285\n",
      " 2985/10000: episode: 104, duration: 0.767s, episode steps:  46, steps per second:  60, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 2.970467, mae: 12.611602, mean_q: 25.398094, mean_eps: 0.866733\n",
      " 3027/10000: episode: 105, duration: 0.700s, episode steps:  42, steps per second:  60, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 2.056835, mae: 12.945086, mean_q: 26.253071, mean_eps: 0.864753\n",
      " 3073/10000: episode: 106, duration: 0.767s, episode steps:  46, steps per second:  60, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 2.777867, mae: 13.184982, mean_q: 26.706186, mean_eps: 0.862772\n",
      " 3107/10000: episode: 107, duration: 0.567s, episode steps:  34, steps per second:  60, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.559 [0.000, 1.000],  loss: 1.796172, mae: 13.299305, mean_q: 26.985578, mean_eps: 0.860973\n",
      " 3134/10000: episode: 108, duration: 0.451s, episode steps:  27, steps per second:  60, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 2.845599, mae: 13.557265, mean_q: 27.524868, mean_eps: 0.859600\n",
      " 3180/10000: episode: 109, duration: 0.801s, episode steps:  46, steps per second:  57, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 3.493989, mae: 13.630895, mean_q: 27.526399, mean_eps: 0.857958\n",
      " 3205/10000: episode: 110, duration: 0.419s, episode steps:  25, steps per second:  60, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 2.394308, mae: 13.491549, mean_q: 27.204728, mean_eps: 0.856360\n",
      " 3312/10000: episode: 111, duration: 1.799s, episode steps: 107, steps per second:  59, episode reward: 107.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 2.962530, mae: 14.240092, mean_q: 28.792954, mean_eps: 0.853390\n",
      " 3332/10000: episode: 112, duration: 0.364s, episode steps:  20, steps per second:  55, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 3.431521, mae: 14.798179, mean_q: 29.993344, mean_eps: 0.850532\n",
      " 3353/10000: episode: 113, duration: 0.356s, episode steps:  21, steps per second:  59, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 3.708820, mae: 14.985952, mean_q: 30.291066, mean_eps: 0.849610\n",
      " 3370/10000: episode: 114, duration: 0.294s, episode steps:  17, steps per second:  58, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 3.090527, mae: 14.804146, mean_q: 29.933573, mean_eps: 0.848755\n",
      " 3420/10000: episode: 115, duration: 0.835s, episode steps:  50, steps per second:  60, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.420 [0.000, 1.000],  loss: 3.097106, mae: 14.996885, mean_q: 30.379558, mean_eps: 0.847248\n",
      " 3497/10000: episode: 116, duration: 1.317s, episode steps:  77, steps per second:  58, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 3.079457, mae: 15.410704, mean_q: 31.273812, mean_eps: 0.844390\n",
      " 3516/10000: episode: 117, duration: 0.316s, episode steps:  19, steps per second:  60, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 2.348055, mae: 15.655662, mean_q: 31.751621, mean_eps: 0.842230\n",
      " 3544/10000: episode: 118, duration: 0.465s, episode steps:  28, steps per second:  60, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 2.884496, mae: 15.509327, mean_q: 31.679979, mean_eps: 0.841172\n",
      " 3581/10000: episode: 119, duration: 0.617s, episode steps:  37, steps per second:  60, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 3.323133, mae: 15.542957, mean_q: 31.584989, mean_eps: 0.839710\n",
      " 3598/10000: episode: 120, duration: 0.282s, episode steps:  17, steps per second:  60, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 4.041437, mae: 15.370599, mean_q: 31.283491, mean_eps: 0.838495\n",
      " 3616/10000: episode: 121, duration: 0.300s, episode steps:  18, steps per second:  60, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.278 [0.000, 1.000],  loss: 2.880224, mae: 15.591984, mean_q: 31.977721, mean_eps: 0.837707\n",
      " 3686/10000: episode: 122, duration: 1.174s, episode steps:  70, steps per second:  60, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.557 [0.000, 1.000],  loss: 3.003482, mae: 16.019972, mean_q: 32.701230, mean_eps: 0.835728\n",
      " 3709/10000: episode: 123, duration: 0.396s, episode steps:  23, steps per second:  58, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 3.226327, mae: 16.103700, mean_q: 32.803648, mean_eps: 0.833635\n",
      " 3728/10000: episode: 124, duration: 0.330s, episode steps:  19, steps per second:  58, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  loss: 2.582516, mae: 16.587177, mean_q: 33.784450, mean_eps: 0.832690\n",
      " 3754/10000: episode: 125, duration: 0.468s, episode steps:  26, steps per second:  56, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.346 [0.000, 1.000],  loss: 2.872660, mae: 16.643529, mean_q: 33.993620, mean_eps: 0.831678\n",
      " 3838/10000: episode: 126, duration: 1.415s, episode steps:  84, steps per second:  59, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.762675, mae: 16.837369, mean_q: 34.484052, mean_eps: 0.829203\n",
      " 3906/10000: episode: 127, duration: 1.189s, episode steps:  68, steps per second:  57, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.544 [0.000, 1.000],  loss: 3.573583, mae: 17.233156, mean_q: 35.258263, mean_eps: 0.825782\n",
      " 3946/10000: episode: 128, duration: 0.696s, episode steps:  40, steps per second:  57, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 4.413198, mae: 18.209038, mean_q: 37.062031, mean_eps: 0.823353\n",
      " 3983/10000: episode: 129, duration: 0.618s, episode steps:  37, steps per second:  60, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 3.693715, mae: 18.050085, mean_q: 36.907913, mean_eps: 0.821620\n",
      " 4007/10000: episode: 130, duration: 0.400s, episode steps:  24, steps per second:  60, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 4.043711, mae: 18.211998, mean_q: 37.206845, mean_eps: 0.820247\n",
      " 4066/10000: episode: 131, duration: 0.982s, episode steps:  59, steps per second:  60, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 4.507786, mae: 18.757726, mean_q: 38.256678, mean_eps: 0.818380\n",
      " 4078/10000: episode: 132, duration: 0.202s, episode steps:  12, steps per second:  60, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 7.653394, mae: 18.360934, mean_q: 37.335420, mean_eps: 0.816782\n",
      " 4093/10000: episode: 133, duration: 0.251s, episode steps:  15, steps per second:  60, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 4.250715, mae: 18.810481, mean_q: 38.334730, mean_eps: 0.816175\n",
      " 4179/10000: episode: 134, duration: 1.432s, episode steps:  86, steps per second:  60, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.580524, mae: 19.173364, mean_q: 38.876432, mean_eps: 0.813902\n",
      " 4212/10000: episode: 135, duration: 0.789s, episode steps:  33, steps per second:  42, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 4.244289, mae: 19.261173, mean_q: 39.149290, mean_eps: 0.811225\n",
      " 4232/10000: episode: 136, duration: 0.343s, episode steps:  20, steps per second:  58, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 6.080419, mae: 19.898530, mean_q: 40.377929, mean_eps: 0.810032\n",
      " 4241/10000: episode: 137, duration: 0.150s, episode steps:   9, steps per second:  60, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 4.904581, mae: 19.751463, mean_q: 40.159766, mean_eps: 0.809380\n",
      " 4271/10000: episode: 138, duration: 0.584s, episode steps:  30, steps per second:  51, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 4.579752, mae: 19.379538, mean_q: 39.688370, mean_eps: 0.808502\n",
      " 4370/10000: episode: 139, duration: 1.749s, episode steps:  99, steps per second:  57, episode reward: 99.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 5.109365, mae: 19.931734, mean_q: 40.532708, mean_eps: 0.805600\n",
      " 4389/10000: episode: 140, duration: 0.316s, episode steps:  19, steps per second:  60, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  loss: 5.193322, mae: 20.238197, mean_q: 41.313567, mean_eps: 0.802945\n",
      " 4426/10000: episode: 141, duration: 0.638s, episode steps:  37, steps per second:  58, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 5.440113, mae: 20.621112, mean_q: 42.086882, mean_eps: 0.801685\n",
      " 4484/10000: episode: 142, duration: 1.032s, episode steps:  58, steps per second:  56, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 4.365449, mae: 20.818481, mean_q: 42.572406, mean_eps: 0.799548\n",
      " 4575/10000: episode: 143, duration: 1.581s, episode steps:  91, steps per second:  58, episode reward: 91.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 5.472273, mae: 21.195801, mean_q: 43.244692, mean_eps: 0.796195\n",
      " 4624/10000: episode: 144, duration: 0.835s, episode steps:  49, steps per second:  59, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 4.638437, mae: 21.591298, mean_q: 44.106502, mean_eps: 0.793045\n",
      " 4658/10000: episode: 145, duration: 0.567s, episode steps:  34, steps per second:  60, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 4.389713, mae: 22.342778, mean_q: 45.707498, mean_eps: 0.791177\n",
      " 4704/10000: episode: 146, duration: 0.765s, episode steps:  46, steps per second:  60, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 4.418675, mae: 22.212373, mean_q: 45.386174, mean_eps: 0.789377\n",
      " 4732/10000: episode: 147, duration: 0.468s, episode steps:  28, steps per second:  60, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 5.844949, mae: 22.435156, mean_q: 45.858049, mean_eps: 0.787713\n",
      " 4809/10000: episode: 148, duration: 1.300s, episode steps:  77, steps per second:  59, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 5.960572, mae: 22.405041, mean_q: 45.691008, mean_eps: 0.785350\n",
      " 4844/10000: episode: 149, duration: 0.582s, episode steps:  35, steps per second:  60, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 4.570696, mae: 23.156151, mean_q: 47.313714, mean_eps: 0.782830\n",
      " 4867/10000: episode: 150, duration: 0.384s, episode steps:  23, steps per second:  60, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 5.032945, mae: 22.921433, mean_q: 46.936027, mean_eps: 0.781525\n",
      " 4953/10000: episode: 151, duration: 1.470s, episode steps:  86, steps per second:  59, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 5.877229, mae: 23.105821, mean_q: 47.291827, mean_eps: 0.779072\n",
      " 4986/10000: episode: 152, duration: 0.548s, episode steps:  33, steps per second:  60, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 7.635732, mae: 23.492688, mean_q: 47.853847, mean_eps: 0.776395\n",
      " 5014/10000: episode: 153, duration: 0.465s, episode steps:  28, steps per second:  60, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 5.489044, mae: 23.645116, mean_q: 48.348956, mean_eps: 0.775023\n",
      " 5056/10000: episode: 154, duration: 0.701s, episode steps:  42, steps per second:  60, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 6.754978, mae: 23.855935, mean_q: 48.903145, mean_eps: 0.773448\n",
      " 5073/10000: episode: 155, duration: 0.286s, episode steps:  17, steps per second:  60, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.765 [0.000, 1.000],  loss: 5.888589, mae: 23.958200, mean_q: 48.866593, mean_eps: 0.772120\n",
      " 5112/10000: episode: 156, duration: 0.648s, episode steps:  39, steps per second:  60, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 7.358223, mae: 24.165444, mean_q: 49.271525, mean_eps: 0.770860\n",
      " 5202/10000: episode: 157, duration: 1.501s, episode steps:  90, steps per second:  60, episode reward: 90.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 6.952664, mae: 24.300727, mean_q: 49.701025, mean_eps: 0.767958\n",
      " 5217/10000: episode: 158, duration: 0.254s, episode steps:  15, steps per second:  59, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 7.680441, mae: 25.362017, mean_q: 51.545935, mean_eps: 0.765595\n",
      " 5242/10000: episode: 159, duration: 0.484s, episode steps:  25, steps per second:  52, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 3.839069, mae: 25.630515, mean_q: 52.696052, mean_eps: 0.764695\n",
      " 5322/10000: episode: 160, duration: 1.345s, episode steps:  80, steps per second:  59, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 8.056924, mae: 25.355140, mean_q: 51.838135, mean_eps: 0.762332\n",
      " 5369/10000: episode: 161, duration: 0.833s, episode steps:  47, steps per second:  56, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  loss: 6.370256, mae: 25.681103, mean_q: 52.520663, mean_eps: 0.759475\n",
      " 5406/10000: episode: 162, duration: 0.617s, episode steps:  37, steps per second:  60, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 6.154000, mae: 25.450646, mean_q: 52.198981, mean_eps: 0.757585\n",
      " 5428/10000: episode: 163, duration: 0.367s, episode steps:  22, steps per second:  60, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  loss: 4.617009, mae: 26.606634, mean_q: 54.888214, mean_eps: 0.756257\n",
      " 5489/10000: episode: 164, duration: 1.017s, episode steps:  61, steps per second:  60, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 6.366201, mae: 26.405172, mean_q: 54.190680, mean_eps: 0.754390\n",
      " 5506/10000: episode: 165, duration: 0.283s, episode steps:  17, steps per second:  60, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 6.716338, mae: 26.960710, mean_q: 54.953339, mean_eps: 0.752635\n",
      " 5551/10000: episode: 166, duration: 0.750s, episode steps:  45, steps per second:  60, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 6.814470, mae: 26.748861, mean_q: 54.844632, mean_eps: 0.751240\n",
      " 5569/10000: episode: 167, duration: 0.300s, episode steps:  18, steps per second:  60, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 6.173662, mae: 26.809779, mean_q: 54.643919, mean_eps: 0.749822\n",
      " 5583/10000: episode: 168, duration: 0.233s, episode steps:  14, steps per second:  60, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 11.413301, mae: 27.156512, mean_q: 55.118496, mean_eps: 0.749103\n",
      " 5626/10000: episode: 169, duration: 0.718s, episode steps:  43, steps per second:  60, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 5.852176, mae: 27.455694, mean_q: 56.079571, mean_eps: 0.747820\n",
      " 5654/10000: episode: 170, duration: 0.466s, episode steps:  28, steps per second:  60, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 8.257041, mae: 27.161197, mean_q: 55.560096, mean_eps: 0.746223\n",
      " 5706/10000: episode: 171, duration: 0.871s, episode steps:  52, steps per second:  60, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 4.983733, mae: 27.315202, mean_q: 55.957104, mean_eps: 0.744422\n",
      " 5804/10000: episode: 172, duration: 1.647s, episode steps:  98, steps per second:  60, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 7.063743, mae: 28.076256, mean_q: 57.440012, mean_eps: 0.741048\n",
      " 5904/10000: episode: 173, duration: 1.668s, episode steps: 100, steps per second:  60, episode reward: 100.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.116613, mae: 28.662132, mean_q: 58.742368, mean_eps: 0.736592\n",
      " 5916/10000: episode: 174, duration: 0.201s, episode steps:  12, steps per second:  60, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 14.408384, mae: 29.162409, mean_q: 59.316741, mean_eps: 0.734072\n",
      " 5944/10000: episode: 175, duration: 0.467s, episode steps:  28, steps per second:  60, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.212553, mae: 28.969161, mean_q: 59.543890, mean_eps: 0.733173\n",
      " 6022/10000: episode: 176, duration: 1.301s, episode steps:  78, steps per second:  60, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 9.279584, mae: 29.371444, mean_q: 60.202137, mean_eps: 0.730788\n",
      " 6078/10000: episode: 177, duration: 0.934s, episode steps:  56, steps per second:  60, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.446 [0.000, 1.000],  loss: 10.022825, mae: 29.825889, mean_q: 60.860043, mean_eps: 0.727773\n",
      " 6091/10000: episode: 178, duration: 0.216s, episode steps:  13, steps per second:  60, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 8.482552, mae: 29.458124, mean_q: 60.635175, mean_eps: 0.726220\n",
      " 6207/10000: episode: 179, duration: 1.934s, episode steps: 116, steps per second:  60, episode reward: 116.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 7.824282, mae: 30.209068, mean_q: 61.787280, mean_eps: 0.723318\n",
      " 6218/10000: episode: 180, duration: 0.183s, episode steps:  11, steps per second:  60, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 6.778578, mae: 30.893165, mean_q: 63.140002, mean_eps: 0.720460\n",
      " 6252/10000: episode: 181, duration: 0.568s, episode steps:  34, steps per second:  60, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 8.679126, mae: 30.991706, mean_q: 63.362905, mean_eps: 0.719448\n",
      " 6273/10000: episode: 182, duration: 0.348s, episode steps:  21, steps per second:  60, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 8.033557, mae: 30.704970, mean_q: 62.866604, mean_eps: 0.718210\n",
      " 6295/10000: episode: 183, duration: 0.371s, episode steps:  22, steps per second:  59, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 3.821737, mae: 30.915853, mean_q: 63.523633, mean_eps: 0.717243\n",
      " 6312/10000: episode: 184, duration: 0.295s, episode steps:  17, steps per second:  58, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 5.952058, mae: 31.158960, mean_q: 63.776517, mean_eps: 0.716365\n",
      " 6337/10000: episode: 185, duration: 0.418s, episode steps:  25, steps per second:  60, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 11.039342, mae: 31.227544, mean_q: 63.789599, mean_eps: 0.715420\n",
      " 6381/10000: episode: 187, duration: 0.515s, episode steps:  31, steps per second:  60, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.581 [0.000, 1.000],  loss: 17.466527, mae: 31.051496, mean_q: 62.879169, mean_eps: 0.713575\n",
      " 6396/10000: episode: 188, duration: 0.250s, episode steps:  15, steps per second:  60, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 9.230678, mae: 31.259597, mean_q: 63.836706, mean_eps: 0.712540\n",
      " 6418/10000: episode: 189, duration: 0.367s, episode steps:  22, steps per second:  60, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 9.638379, mae: 30.836355, mean_q: 62.733006, mean_eps: 0.711708\n",
      " 6480/10000: episode: 190, duration: 1.034s, episode steps:  62, steps per second:  60, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.419 [0.000, 1.000],  loss: 9.331738, mae: 31.244046, mean_q: 63.626830, mean_eps: 0.709817\n",
      " 6570/10000: episode: 191, duration: 1.499s, episode steps:  90, steps per second:  60, episode reward: 90.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 8.648709, mae: 31.758465, mean_q: 64.694119, mean_eps: 0.706397\n",
      " 6747/10000: episode: 192, duration: 2.954s, episode steps: 177, steps per second:  60, episode reward: 177.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 8.519831, mae: 32.422903, mean_q: 66.240322, mean_eps: 0.700390\n",
      " 6775/10000: episode: 193, duration: 0.468s, episode steps:  28, steps per second:  60, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.744749, mae: 32.747322, mean_q: 66.867116, mean_eps: 0.695778\n",
      " 6790/10000: episode: 194, duration: 0.249s, episode steps:  15, steps per second:  60, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 8.000595, mae: 32.047778, mean_q: 65.794503, mean_eps: 0.694810\n",
      " 6812/10000: episode: 195, duration: 0.366s, episode steps:  22, steps per second:  60, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 9.889320, mae: 33.020899, mean_q: 67.200116, mean_eps: 0.693978\n",
      " 6833/10000: episode: 196, duration: 0.349s, episode steps:  21, steps per second:  60, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.381 [0.000, 1.000],  loss: 8.429202, mae: 32.870348, mean_q: 67.395283, mean_eps: 0.693010\n",
      " 6882/10000: episode: 197, duration: 0.818s, episode steps:  49, steps per second:  60, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 14.558390, mae: 32.939719, mean_q: 67.317201, mean_eps: 0.691435\n",
      " 6899/10000: episode: 198, duration: 0.284s, episode steps:  17, steps per second:  60, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 9.344324, mae: 33.358163, mean_q: 68.152011, mean_eps: 0.689950\n",
      " 6939/10000: episode: 199, duration: 0.700s, episode steps:  40, steps per second:  57, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 8.734063, mae: 33.707279, mean_q: 68.871136, mean_eps: 0.688667\n",
      " 6952/10000: episode: 200, duration: 0.217s, episode steps:  13, steps per second:  60, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 9.166036, mae: 33.456059, mean_q: 68.877238, mean_eps: 0.687475\n",
      " 7018/10000: episode: 201, duration: 1.101s, episode steps:  66, steps per second:  60, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 8.227624, mae: 33.344180, mean_q: 68.308464, mean_eps: 0.685697\n",
      " 7045/10000: episode: 202, duration: 0.448s, episode steps:  27, steps per second:  60, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.593 [0.000, 1.000],  loss: 12.555831, mae: 33.654596, mean_q: 68.719903, mean_eps: 0.683605\n",
      " 7141/10000: episode: 203, duration: 1.602s, episode steps:  96, steps per second:  60, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 8.238687, mae: 33.930454, mean_q: 69.625193, mean_eps: 0.680837\n",
      " 7169/10000: episode: 204, duration: 0.467s, episode steps:  28, steps per second:  60, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.607 [0.000, 1.000],  loss: 10.816488, mae: 34.243345, mean_q: 70.524387, mean_eps: 0.678048\n",
      " 7186/10000: episode: 205, duration: 0.282s, episode steps:  17, steps per second:  60, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: 17.462020, mae: 33.470335, mean_q: 68.617739, mean_eps: 0.677035\n",
      " 7200/10000: episode: 206, duration: 0.235s, episode steps:  14, steps per second:  60, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 10.380840, mae: 34.487605, mean_q: 70.370901, mean_eps: 0.676338\n",
      " 7246/10000: episode: 207, duration: 0.766s, episode steps:  46, steps per second:  60, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 7.825756, mae: 34.464848, mean_q: 70.856206, mean_eps: 0.674987\n",
      " 7268/10000: episode: 208, duration: 0.366s, episode steps:  22, steps per second:  60, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.318 [0.000, 1.000],  loss: 15.403399, mae: 34.397580, mean_q: 70.275273, mean_eps: 0.673458\n",
      " 7314/10000: episode: 209, duration: 0.767s, episode steps:  46, steps per second:  60, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.188807, mae: 34.553269, mean_q: 70.996064, mean_eps: 0.671928\n",
      " 7361/10000: episode: 210, duration: 0.784s, episode steps:  47, steps per second:  60, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  loss: 8.443325, mae: 34.848337, mean_q: 71.622387, mean_eps: 0.669835\n",
      " 7442/10000: episode: 211, duration: 1.351s, episode steps:  81, steps per second:  60, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 10.379042, mae: 35.214168, mean_q: 72.183309, mean_eps: 0.666955\n",
      " 7462/10000: episode: 212, duration: 0.334s, episode steps:  20, steps per second:  60, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 9.586347, mae: 35.306908, mean_q: 72.582998, mean_eps: 0.664683\n",
      " 7476/10000: episode: 213, duration: 0.233s, episode steps:  14, steps per second:  60, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 5.466132, mae: 35.168082, mean_q: 72.550918, mean_eps: 0.663917\n",
      " 7543/10000: episode: 214, duration: 1.117s, episode steps:  67, steps per second:  60, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 12.350968, mae: 35.553854, mean_q: 72.827713, mean_eps: 0.662095\n",
      " 7713/10000: episode: 215, duration: 2.887s, episode steps: 170, steps per second:  59, episode reward: 170.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 11.504082, mae: 35.967197, mean_q: 73.606760, mean_eps: 0.656763\n",
      " 7763/10000: episode: 216, duration: 0.835s, episode steps:  50, steps per second:  60, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 10.218627, mae: 36.336729, mean_q: 74.101302, mean_eps: 0.651812\n",
      " 7921/10000: episode: 217, duration: 2.635s, episode steps: 158, steps per second:  60, episode reward: 158.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 9.485525, mae: 36.584647, mean_q: 74.815752, mean_eps: 0.647132\n",
      " 7978/10000: episode: 218, duration: 0.949s, episode steps:  57, steps per second:  60, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 7.522275, mae: 37.326176, mean_q: 76.502361, mean_eps: 0.642295\n",
      " 8046/10000: episode: 219, duration: 1.133s, episode steps:  68, steps per second:  60, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 9.736989, mae: 37.335113, mean_q: 76.760678, mean_eps: 0.639482\n",
      " 8056/10000: episode: 220, duration: 0.166s, episode steps:  10, steps per second:  60, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 10.521392, mae: 38.680795, mean_q: 78.930881, mean_eps: 0.637728\n",
      " 8190/10000: episode: 221, duration: 2.237s, episode steps: 134, steps per second:  60, episode reward: 134.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 14.314962, mae: 37.968248, mean_q: 77.800973, mean_eps: 0.634487\n",
      " 8255/10000: episode: 222, duration: 1.085s, episode steps:  65, steps per second:  60, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 13.121142, mae: 38.422388, mean_q: 78.550637, mean_eps: 0.630010\n",
      " 8268/10000: episode: 223, duration: 0.266s, episode steps:  13, steps per second:  49, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 14.365320, mae: 38.004167, mean_q: 78.140846, mean_eps: 0.628255\n",
      " 8442/10000: episode: 224, duration: 2.968s, episode steps: 174, steps per second:  59, episode reward: 174.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 11.302809, mae: 38.600407, mean_q: 79.080714, mean_eps: 0.624047\n",
      " 8480/10000: episode: 225, duration: 0.633s, episode steps:  38, steps per second:  60, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  loss: 11.325861, mae: 39.320822, mean_q: 80.396191, mean_eps: 0.619278\n",
      " 8530/10000: episode: 226, duration: 0.835s, episode steps:  50, steps per second:  60, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 11.085425, mae: 39.083910, mean_q: 79.998637, mean_eps: 0.617298\n",
      " 8610/10000: episode: 227, duration: 1.369s, episode steps:  80, steps per second:  58, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 11.092508, mae: 39.272492, mean_q: 80.503303, mean_eps: 0.614372\n",
      " 8638/10000: episode: 228, duration: 0.481s, episode steps:  28, steps per second:  58, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.393 [0.000, 1.000],  loss: 9.101057, mae: 39.814290, mean_q: 81.859990, mean_eps: 0.611942\n",
      " 8716/10000: episode: 229, duration: 1.353s, episode steps:  78, steps per second:  58, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.436 [0.000, 1.000],  loss: 13.219414, mae: 39.815276, mean_q: 81.646561, mean_eps: 0.609557\n",
      " 8764/10000: episode: 230, duration: 0.819s, episode steps:  48, steps per second:  59, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 17.455719, mae: 39.595544, mean_q: 81.373002, mean_eps: 0.606722\n",
      " 8894/10000: episode: 231, duration: 2.181s, episode steps: 130, steps per second:  60, episode reward: 130.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 12.290304, mae: 40.512757, mean_q: 83.216770, mean_eps: 0.602718\n",
      " 8938/10000: episode: 232, duration: 0.735s, episode steps:  44, steps per second:  60, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 14.153394, mae: 39.864996, mean_q: 81.900346, mean_eps: 0.598803\n",
      " 9020/10000: episode: 233, duration: 1.366s, episode steps:  82, steps per second:  60, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 12.955416, mae: 40.746794, mean_q: 83.659231, mean_eps: 0.595967\n",
      " 9106/10000: episode: 234, duration: 1.487s, episode steps:  86, steps per second:  58, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.453 [0.000, 1.000],  loss: 13.992232, mae: 41.077106, mean_q: 84.251499, mean_eps: 0.592187\n",
      " 9220/10000: episode: 235, duration: 1.915s, episode steps: 114, steps per second:  60, episode reward: 114.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 10.448946, mae: 42.043928, mean_q: 86.243234, mean_eps: 0.587687\n",
      " 9356/10000: episode: 236, duration: 2.269s, episode steps: 136, steps per second:  60, episode reward: 136.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 12.358232, mae: 42.321887, mean_q: 86.872661, mean_eps: 0.582063\n",
      " 9381/10000: episode: 237, duration: 0.420s, episode steps:  25, steps per second:  59, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 9.388173, mae: 42.542959, mean_q: 87.919373, mean_eps: 0.578440\n",
      " 9403/10000: episode: 238, duration: 0.381s, episode steps:  22, steps per second:  58, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  loss: 7.792266, mae: 42.444037, mean_q: 87.715301, mean_eps: 0.577382\n",
      " 9478/10000: episode: 239, duration: 1.307s, episode steps:  75, steps per second:  57, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 12.674387, mae: 42.659290, mean_q: 87.826891, mean_eps: 0.575200\n",
      " 9527/10000: episode: 240, duration: 0.829s, episode steps:  49, steps per second:  59, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.449 [0.000, 1.000],  loss: 10.940560, mae: 43.034548, mean_q: 88.447600, mean_eps: 0.572410\n",
      " 9567/10000: episode: 241, duration: 0.680s, episode steps:  40, steps per second:  59, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.425 [0.000, 1.000],  loss: 15.732872, mae: 43.114871, mean_q: 88.808184, mean_eps: 0.570407\n",
      " 9742/10000: episode: 242, duration: 2.953s, episode steps: 175, steps per second:  59, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 11.736851, mae: 43.914510, mean_q: 90.185445, mean_eps: 0.565570\n",
      " 9779/10000: episode: 243, duration: 0.621s, episode steps:  37, steps per second:  60, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 14.521321, mae: 44.278186, mean_q: 90.606986, mean_eps: 0.560800\n",
      " 9845/10000: episode: 244, duration: 1.130s, episode steps:  66, steps per second:  58, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 12.650118, mae: 44.724197, mean_q: 91.541397, mean_eps: 0.558483\n",
      " 9941/10000: episode: 245, duration: 1.600s, episode steps:  96, steps per second:  60, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 14.010314, mae: 44.903356, mean_q: 92.255875, mean_eps: 0.554837\n",
      "done, took 169.908 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x15977582820>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the agent\n",
    "dqn.fit(env, nb_steps=10000, visualize=True, verbose=2)\n",
    "\n",
    "# # Evaluate the agent\n",
    "# dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "745ddd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10000 episodes ...\n",
      "Episode 1: reward: 404.000, steps: 404\n",
      "Episode 2: reward: 210.000, steps: 210\n",
      "Episode 3: reward: 236.000, steps: 236\n",
      "Episode 4: reward: 206.000, steps: 206\n",
      "Episode 5: reward: 234.000, steps: 234\n",
      "Episode 6: reward: 190.000, steps: 190\n",
      "Episode 7: reward: 202.000, steps: 202\n",
      "Episode 8: reward: 219.000, steps: 219\n",
      "Episode 9: reward: 269.000, steps: 269\n",
      "Episode 10: reward: 285.000, steps: 285\n",
      "Episode 11: reward: 306.000, steps: 306\n",
      "Episode 12: reward: 264.000, steps: 264\n",
      "Episode 13: reward: 255.000, steps: 255\n",
      "Episode 14: reward: 241.000, steps: 241\n",
      "Episode 15: reward: 250.000, steps: 250\n",
      "Episode 16: reward: 204.000, steps: 204\n",
      "Episode 17: reward: 189.000, steps: 189\n",
      "Episode 18: reward: 271.000, steps: 271\n",
      "Episode 19: reward: 251.000, steps: 251\n",
      "Episode 20: reward: 210.000, steps: 210\n",
      "Episode 21: reward: 324.000, steps: 324\n",
      "Episode 22: reward: 208.000, steps: 208\n",
      "Episode 23: reward: 208.000, steps: 208\n",
      "Episode 24: reward: 236.000, steps: 236\n",
      "Episode 25: reward: 182.000, steps: 182\n",
      "Episode 26: reward: 194.000, steps: 194\n",
      "Episode 27: reward: 233.000, steps: 233\n",
      "Episode 28: reward: 183.000, steps: 183\n",
      "Episode 29: reward: 234.000, steps: 234\n",
      "Episode 30: reward: 291.000, steps: 291\n",
      "Episode 31: reward: 415.000, steps: 415\n",
      "Episode 32: reward: 176.000, steps: 176\n",
      "Episode 33: reward: 211.000, steps: 211\n",
      "Episode 34: reward: 253.000, steps: 253\n",
      "Episode 35: reward: 217.000, steps: 217\n",
      "Episode 36: reward: 192.000, steps: 192\n",
      "Episode 37: reward: 203.000, steps: 203\n",
      "Episode 38: reward: 198.000, steps: 198\n",
      "Episode 39: reward: 272.000, steps: 272\n",
      "Episode 40: reward: 195.000, steps: 195\n",
      "Episode 41: reward: 232.000, steps: 232\n",
      "Episode 42: reward: 216.000, steps: 216\n",
      "Episode 43: reward: 232.000, steps: 232\n",
      "Episode 44: reward: 232.000, steps: 232\n",
      "Episode 45: reward: 346.000, steps: 346\n",
      "Episode 46: reward: 257.000, steps: 257\n",
      "Episode 47: reward: 254.000, steps: 254\n",
      "Episode 48: reward: 178.000, steps: 178\n",
      "Episode 49: reward: 240.000, steps: 240\n",
      "Episode 50: reward: 242.000, steps: 242\n",
      "Episode 51: reward: 209.000, steps: 209\n",
      "Episode 52: reward: 211.000, steps: 211\n",
      "Episode 53: reward: 193.000, steps: 193\n",
      "Episode 54: reward: 237.000, steps: 237\n",
      "Episode 55: reward: 217.000, steps: 217\n",
      "Episode 56: reward: 208.000, steps: 208\n",
      "Episode 57: reward: 219.000, steps: 219\n",
      "Episode 58: reward: 215.000, steps: 215\n",
      "Episode 59: reward: 209.000, steps: 209\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Evaluate the agent\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mdqn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnb_episodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvisualize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\books\\django\\environment\\lib\\site-packages\\rl\\core.py:352\u001B[0m, in \u001B[0;36mAgent.test\u001B[1;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001B[0m\n\u001B[0;32m    350\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocessor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    351\u001B[0m     observation, r, d, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocessor\u001B[38;5;241m.\u001B[39mprocess_step(observation, r, d, info)\n\u001B[1;32m--> 352\u001B[0m \u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mon_action_end\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    353\u001B[0m reward \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m r\n\u001B[0;32m    354\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m info\u001B[38;5;241m.\u001B[39mitems():\n",
      "File \u001B[1;32m~\\Desktop\\books\\django\\environment\\lib\\site-packages\\rl\\callbacks.py:98\u001B[0m, in \u001B[0;36mCallbackList.on_action_end\u001B[1;34m(self, action, logs)\u001B[0m\n\u001B[0;32m     96\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m callback \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallbacks:\n\u001B[0;32m     97\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(\u001B[38;5;28mgetattr\u001B[39m(callback, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mon_action_end\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)):\n\u001B[1;32m---> 98\u001B[0m         \u001B[43mcallback\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mon_action_end\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\books\\django\\environment\\lib\\site-packages\\rl\\callbacks.py:360\u001B[0m, in \u001B[0;36mVisualizer.on_action_end\u001B[1;34m(self, action, logs)\u001B[0m\n\u001B[0;32m    358\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mon_action_end\u001B[39m(\u001B[38;5;28mself\u001B[39m, action, logs):\n\u001B[0;32m    359\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\" Render environment at the end of each action \"\"\"\u001B[39;00m\n\u001B[1;32m--> 360\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mhuman\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\books\\django\\environment\\lib\\site-packages\\gym\\core.py:233\u001B[0m, in \u001B[0;36mWrapper.render\u001B[1;34m(self, mode, **kwargs)\u001B[0m\n\u001B[0;32m    232\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrender\u001B[39m(\u001B[38;5;28mself\u001B[39m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 233\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mrender(mode, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Desktop\\books\\django\\environment\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:213\u001B[0m, in \u001B[0;36mCartPoleEnv.render\u001B[1;34m(self, mode)\u001B[0m\n\u001B[0;32m    210\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcarttrans\u001B[38;5;241m.\u001B[39mset_translation(cartx, carty)\n\u001B[0;32m    211\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpoletrans\u001B[38;5;241m.\u001B[39mset_rotation(\u001B[38;5;241m-\u001B[39mx[\u001B[38;5;241m2\u001B[39m])\n\u001B[1;32m--> 213\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mviewer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreturn_rgb_array\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrgb_array\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\books\\django\\environment\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py:124\u001B[0m, in \u001B[0;36mViewer.render\u001B[1;34m(self, return_rgb_array)\u001B[0m\n\u001B[0;32m    122\u001B[0m     arr \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mreshape(buffer\u001B[38;5;241m.\u001B[39mheight, buffer\u001B[38;5;241m.\u001B[39mwidth, \u001B[38;5;241m4\u001B[39m)\n\u001B[0;32m    123\u001B[0m     arr \u001B[38;5;241m=\u001B[39m arr[::\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,:,\u001B[38;5;241m0\u001B[39m:\u001B[38;5;241m3\u001B[39m]\n\u001B[1;32m--> 124\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwindow\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflip\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    125\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39monetime_geoms \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    126\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m arr \u001B[38;5;28;01mif\u001B[39;00m return_rgb_array \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39misopen\n",
      "File \u001B[1;32m~\\Desktop\\books\\django\\environment\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py:338\u001B[0m, in \u001B[0;36mWin32Window.flip\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    335\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_interval:\n\u001B[0;32m    336\u001B[0m             _dwmapi\u001B[38;5;241m.\u001B[39mDwmFlush()\n\u001B[1;32m--> 338\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflip\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\books\\django\\environment\\lib\\site-packages\\pyglet\\gl\\win32.py:252\u001B[0m, in \u001B[0;36mWin32Context.flip\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    251\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mflip\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m--> 252\u001B[0m     \u001B[43m_gdi32\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSwapBuffers\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcanvas\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhdc\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Evaluate the agent\n",
    "dqn.test(env, nb_episodes=5, visualize=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}